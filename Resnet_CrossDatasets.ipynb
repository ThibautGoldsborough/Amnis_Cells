{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72658802",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import required packages\n",
    "import cv2 as cv\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from Helper_functions import load_dict\n",
    "\n",
    "from AI_functions import resnet18,CellDataset_supervised,data_generator\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from Helper_functions import log_pol_scale, log_pol\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3458ee27",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETS=[\"Retina_1_2\",\"Retina_0_0\",\"Colon\",\"Choroid\",\"BoneMarrow_sample1\"]\n",
    "\n",
    "LABELS_DICT={}\n",
    "\n",
    "\n",
    "train_data_list=[]\n",
    "test_data_list=[]\n",
    "\n",
    "cutoff=30000\n",
    "\n",
    "\n",
    "for N,DATASET in enumerate(DATASETS):\n",
    "    basepath = r\"C:\\Users\\Thibaut Goldsborough\\Documents\\Seth_BoneMarrow\\Data\\\\\" +DATASET\n",
    "    outpath = basepath + \"\\\\Outputs\"\n",
    "    image_dim=64 #Dim of the final images\n",
    "    nuclear_channel=\"Ch7\"\n",
    "    cellmask_channel=\"Ch1_mask\"\n",
    "    df=pd.read_csv(outpath+\"\\\\cell_info1.csv\")\n",
    "    cell_names=df[\"Cell_ID\"].to_numpy()\n",
    "    Prediction_Channels=['Ch07']\n",
    "\n",
    "    image_dict=load_dict(outpath,cell_names,image_dim)\n",
    "\n",
    "    Channels=['Ch1']  #Channel to be fed to the NN\n",
    "    images_with_index = []\n",
    "\n",
    "    for image_i in image_dict:\n",
    "        #print(image_dict[image_i].keys())\n",
    "        if len(image_dict[image_i].keys())>=len(Channels):\n",
    "            image=cv.merge([image_dict[image_i][i] for i in Channels])\n",
    "            images_with_index.append((int(image_i),image))\n",
    "        else:\n",
    "            print(image_i)\n",
    "        \n",
    "    images=np.array([image[1] for image in images_with_index])\n",
    "    names=np.array([image[0] for image in images_with_index])\n",
    "    assert sum(names!=df['Cell_ID'].to_numpy()) ==0  #Check that the order has been preserved\n",
    "    DNA_pos=df[\"DNA_pos\"].to_numpy()\n",
    "    Touches_Boundary=df[\"Touches_boundary\"].to_numpy()\n",
    "    tissue=np.array([N for i in names]) #Labels are the tissues\n",
    "    # labels=df[[\"Intensity_MC_\"+channel for channel in Prediction_Channels]].to_numpy()\n",
    "    labels=df[[\"Scaled_\"+channel for channel in Prediction_Channels]].to_numpy()\n",
    "    # #labels=log_pol_scale(labels,slope=1,c=1000)\n",
    "    # logs=log_pol(labels,slope=1,c=1000)\n",
    "    # labels=(logs-6.21703)/1.7187  #These were determined using a custom script\n",
    "\n",
    "    Thresh=50\n",
    "    plt.hist(df[\"Gradient RMS_M01_Ch01\"],bins=200);\n",
    "    plt.axvline(x=Thresh,color=\"red\")\n",
    "    plt.show()\n",
    "    idx_to_keep=np.array(Touches_Boundary==0,dtype=int)+np.array(df[\"Gradient RMS_M01_Ch01\"]>Thresh,dtype=int)==2 #np.array(DNA_pos==1,dtype=int)+\n",
    "    #Filter\n",
    "    print(len(images))\n",
    "    images=images[idx_to_keep]\n",
    "    names=names[idx_to_keep]\n",
    "    labels=labels[idx_to_keep]\n",
    "\n",
    "    images=images[:cutoff]\n",
    "    names=names[:cutoff]\n",
    "    labels=labels[:cutoff]\n",
    "\n",
    "    LABELS_DICT[DATASET]=labels\n",
    "\n",
    "    plt.hist(labels[:,0],bins=100);\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    mini=int(round(abs(np.array(images).min()),0))\n",
    "    images=images+abs(np.array(images).min())\n",
    "    mean=np.array(images).mean()\n",
    "    maxi=np.array(images).max()\n",
    "    std=np.array(images).std()\n",
    "\n",
    "    print(len(images))\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    returned=data_generator(images,labels,names,mini,train_test_split = 0.8,batch_size = 100,sample=False)\n",
    "\n",
    "    \n",
    "    train,test,batch_size,mean_loader,std_loader=returned\n",
    "    print(mean_loader,std_loader,mini)\n",
    "\n",
    "   # mean_loader=0.2\n",
    "   # std_loader=0.02\n",
    "    [train_data,train_data1,train_labels,train_ID]=train\n",
    "    [test_data,test_data1,test_labels,test_ID]=test\n",
    "    transform_train = transforms.Compose(\n",
    "        [transforms.ToPILImage('L'),transforms.RandomHorizontalFlip(p=0.5),transforms.RandomVerticalFlip(p=0.5),\n",
    "        transforms.RandomRotation(degrees=18,fill=mini),\n",
    "      #  transforms.Lambda(lambda x: np.array(x) -mini ),  #This was added on 12/07\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean_loader,std_loader)  ,\n",
    "        transforms.Lambda(lambda x: x-x[0,0] ), \n",
    "        ])\n",
    "\n",
    "    transform_validation = transforms.Compose(\n",
    "        [transforms.ToPILImage('L'),\n",
    " \n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean_loader,std_loader),\n",
    "        transforms.Lambda(lambda x: x-x[0,0] ), \n",
    "        ])\n",
    "\n",
    "\n",
    "    train_data = CellDataset_supervised(train_data1,train_labels,train_ID, transform_train)\n",
    "    test_data = CellDataset_supervised(test_data1,test_labels, test_ID,transform_validation)\n",
    "\n",
    "    train_data_list.append(train_data)\n",
    "    test_data_list.append(test_data)\n",
    "\n",
    "\n",
    "\n",
    "from torch.utils.data import ConcatDataset\n",
    "train_cat = ConcatDataset([train_data for train_data in train_data_list])\n",
    "train_cat_loader = DataLoader(train_cat, batch_size=524,shuffle=True,drop_last=True)\n",
    "\n",
    "\n",
    "test_cat = ConcatDataset([test_data for test_data in test_data_list])\n",
    "test_cat_loader = DataLoader(test_cat, batch_size=524,shuffle=True)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22057ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from Helper_functions import log_pol\n",
    "# for N,DATASET in enumerate(DATASETS):\n",
    "#     basepath = r\"C:\\Users\\Thibaut Goldsborough\\Documents\\Seth_BoneMarrow\\Data\\\\\" +DATASET\n",
    "#     outpath = basepath + \"\\\\Outputs\"\n",
    "#     df=pd.read_csv(outpath+\"\\\\cell_info.csv\")\n",
    "#     labels=df[[\"Intensity_MC_\"+channel for channel in Prediction_Channels]].to_numpy()\n",
    "#     logs=log_pol(labels,slope=1,c=1000)\n",
    "#     if N==0:\n",
    "#         LABELS=logs.copy()\n",
    "#     else:\n",
    "#         print(np.shape(LABELS),np.shape(logs))\n",
    "#         LABELS=np.vstack((LABELS,logs))\n",
    "#     plt.hist(logs,bins=100);\n",
    "#     plt.show()\n",
    "\n",
    "# plt.hist(LABELS,bins=1000);\n",
    "# print(np.mean(LABELS))\n",
    "# print(np.std(LABELS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90c0631",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(NN, device, dataloader, loss_fn, optimizer,noise_factor=0):\n",
    "    NN.train()\n",
    "    train_loss = []\n",
    "    # Iterate the dataloader (we do not need the label values, this is unsupervised learning)\n",
    "    for image_batch,labels_batch,_ in dataloader: # with \"_\" we just ignore the labels (the second element of the dataloader tuple)\n",
    "        image_noisy = image_batch\n",
    "        image_batch = image_noisy.to(device)\n",
    "        labels_batch=labels_batch.to(device)\n",
    "        output = NN(image_batch)\n",
    "        # Evaluate loss\n",
    "        loss = loss_fn(labels_batch,output)\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss.append(loss.detach().cpu().numpy())\n",
    "    return np.mean(train_loss)\n",
    "\n",
    "def spearman(x,y):\n",
    "    vx = x - torch.mean(x)\n",
    "    vy = y - torch.mean(y)\n",
    "    cost = torch.sum(vx * vy) / (torch.sqrt(torch.sum(vx ** 2)) * torch.sqrt(torch.sum(vy ** 2)))\n",
    "    return cost\n",
    "\n",
    "### validationing function\n",
    "def validation_epoch(NN, device, dataloader, loss_fn):\n",
    "    NN.eval()\n",
    "    val_loss=[]\n",
    "    with torch.no_grad(): # No need to track the gradients\n",
    "        for image_batch,labels_batch,ID_batch in dataloader:\n",
    "            # Move tensor to the proper device\n",
    "            image_batch = image_batch.to(device)\n",
    "            labels_batch = labels_batch.to(device)\n",
    "            output = NN(image_batch)\n",
    "         #   loss = loss_fn(labels_batch,output,)\n",
    "            loss=spearman(labels_batch,output)\n",
    "            val_loss.append(loss.detach().cpu().numpy())\n",
    "\n",
    "    return np.mean(val_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa3eb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_results_matrix=np.zeros((len(train_data_list),len(test_data_list)))\n",
    "test_results_matrix=np.zeros((len(train_data_list),len(test_data_list)))\n",
    "num_epochs=10\n",
    "lr=1e-3\n",
    "num_classes=len(Prediction_Channels)\n",
    "\n",
    "models=[]\n",
    "\n",
    "for i,_ in enumerate(train_data_list):\n",
    "    print(DATASETS[i])\n",
    "    ConvNet_simple=resnet18(channel_num=len(Channels),num_classes=num_classes)\n",
    "    #ConvNet_simple=Trained_model\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    ConvNet_simple.to(device)\n",
    "    loss_dict={\"L1\":nn.L1Loss(),\"MSE\":nn.MSELoss()}\n",
    "\n",
    "    loss_fn = loss_dict[\"L1\"]\n",
    "    #optimizer = optim.Adam(ConvNet_simple.parameters(), lr = lr) \n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, ConvNet_simple.parameters()),lr=lr)\n",
    "    diz_loss = {'train_loss':[],'val_loss':[]}\n",
    "\n",
    "    train_loader = DataLoader(train_data_list[i], batch_size=128,shuffle=True,drop_last=True)\n",
    "    print(\"background is \",next(iter(train_loader))[0][0][0][0][0])\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        train_loss = train_epoch(ConvNet_simple,device,train_loader,loss_fn,optimizer)\n",
    "\n",
    "        if epoch > num_epochs-3:\n",
    "            for j,_ in enumerate(test_data_list):\n",
    "                validation_loader = DataLoader(test_data_list[j], batch_size=128,shuffle=True,drop_last=True)\n",
    "                val_loss = validation_epoch(ConvNet_simple,device,validation_loader,loss_fn)\n",
    "\n",
    "                train_results_matrix[i,j]+=train_loss\n",
    "                test_results_matrix[i,j]+=val_loss\n",
    "                print(DATASETS[j],val_loss)\n",
    "            fig, ax = plt.subplots(figsize=(7,7))\n",
    "        print('\\n EPOCH',epoch+1,' \\t train loss',train_loss)\n",
    "    ax.set_aspect('equal')\n",
    "    LABELS=[\"Retina\",\"Colon\",\"Choroid\",\"BM\"]\n",
    "    s=sns.heatmap(test_results_matrix/2,xticklabels=LABELS,yticklabels=LABELS,cmap='Spectral_r',cbar_kws={'label': 'L1 Loss','shrink':0.7})\n",
    "    s.set(xlabel='Test Datasets', ylabel='Train Datasets')\n",
    "    plt.show()\n",
    "\n",
    "    models.append(ConvNet_simple)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4bfd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETS=[\"Retina_1_2\",\"Colon\",\"Choroid\",\"BoneMarrow_sample1\",\"Retina_0_0\"]\n",
    "IMS=[]\n",
    "OUTS=[]\n",
    "INPUTS=[]\n",
    "for j,test in enumerate(test_data_list):\n",
    "    validation_loader = DataLoader(test_data_list[j], batch_size=128,shuffle=True,drop_last=True)\n",
    "    for i,model in enumerate(models):\n",
    "        inputs=[]\n",
    "        outputs=[]\n",
    "        images=[]\n",
    "        with torch.no_grad(): # No need to track the gradients\n",
    "                for image_batch,labels_batch,ID_batch in validation_loader:\n",
    "                    # Move tensor to the proper device\n",
    "                    image_batch = image_batch.to(device)\n",
    "                    labels_batch = labels_batch.to(device)\n",
    "                    output = model(image_batch)\n",
    "                    inputs+=list(labels_batch.cpu().flatten().numpy())\n",
    "                    outputs+=list(output.cpu().flatten().numpy())\n",
    "                    images+=list(image_batch.cpu().numpy())\n",
    "        \n",
    "        # plt.figure(figsize=(10,10))\n",
    "        # plt.scatter(torch.Tensor(inputs),torch.Tensor(outputs),s=0.5)\n",
    "        # plt.title(str(\"Trained on:\"+DATASETS[i]+\"Tested on:\"+DATASETS[j]))\n",
    "        # plt.show()\n",
    "        # print(spearman(torch.Tensor(inputs),torch.Tensor(outputs)))\n",
    "\n",
    "        if i==4 and j==3:\n",
    "            IMS.append(images)\n",
    "            OUTS.append(outputs)\n",
    "            INPUTS.append(inputs)\n",
    "    print(len(np.unique(image_batch.cpu().flatten().numpy())))\n",
    "    plt.hist(inputs,bins=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b55637d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#plt.hist(INPUTS[0],bins=100);\n",
    "plt.hist(OUTS[0],bins=100);\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(INPUTS[0],OUTS[0],s=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34b0deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=np.array(IMS[0])[np.array(OUTS[0])>0.25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10b60f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(23):\n",
    "    fig,(ax1,ax2)=plt.subplots(1,2)\n",
    "    ax1.imshow(a[i][0],vmin=-10,vmax=10)\n",
    "    print(a[i][0][0,0])\n",
    "    ax2.imshow(IMS[0][i][0],vmin=-10,vmax=10)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3b59b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=np.zeros(np.shape(inputs))\n",
    "a+=np.mean(inputs)\n",
    "spearman(torch.Tensor(a),torch.Tensor(outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97425f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(inputs,outputs,s=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81289016",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7,7))\n",
    "ax.set_aspect('equal')\n",
    "LABELS=[\"Retina_1\",\"Retina_0\",\"Colon\",\"Choroid\",\"BM\"]\n",
    "s=sns.heatmap(test_results_matrix/2,xticklabels=LABELS,yticklabels=LABELS,cmap='Spectral_r',cbar_kws={'label': 'Spearman correlation','shrink':0.7},annot=True,linewidths=.5)\n",
    "s.set(xlabel='Test Datasets', ylabel='Train Datasets')\n",
    "#plt.savefig(\"conf_mat_tissues_corr.jpg\",dpi=1000,bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792d3035",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "96e92283253362575e1b2577a58171a1def071c2d4840c376515c402fb1735d8"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('deep_learning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
