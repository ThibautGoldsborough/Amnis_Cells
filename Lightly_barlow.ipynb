{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import required packages\n",
    "import cv2 as cv\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "from Helper_functions import load_dict\n",
    "\n",
    "from AI_functions import resnet18,data_generator\n",
    "\n",
    "from AI_functions import resnet18, BarlowTwins, ImageCollateFunction,BarlowTwinsLoss,CellDataset,data_generator\n",
    "\n",
    "latent_dim=512\n",
    "epochs=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom dataset\n",
    "class CellDataset():\n",
    "    def __init__(self, images,labels,ID, transforms=None):\n",
    "        self.X = images\n",
    "        self.Y=  labels\n",
    "        self.Z= ID\n",
    "        self.transforms = transforms\n",
    "         \n",
    "    def __len__(self):\n",
    "        return (len(self.X))\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        data = self.X[i]\n",
    "        label=self.Y[i]\n",
    "        ID=self.Z[i]\n",
    "        data = np.asarray(data).astype(np.uint8)\n",
    "\n",
    "        if self.transforms:\n",
    "            data1 = self.transforms(data)\n",
    "            data2 = self.transforms(data)\n",
    "        \n",
    "        return (data1,data2),np.array((label,ID))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETS=[\"Retina_1_2\",\"Colon\",\"Choroid\",\"BoneMarrow_sample1\"]\n",
    "\n",
    "train_data_list=[]\n",
    "test_data_list=[]\n",
    "\n",
    "cutoff=30000\n",
    "\n",
    "\n",
    "for N,DATASET in enumerate(DATASETS):\n",
    "    basepath = r\"C:\\Users\\Thibaut Goldsborough\\Documents\\Seth_BoneMarrow\\Data\\\\\" +DATASET\n",
    "    outpath = basepath + \"\\\\Outputs\"\n",
    "    image_dim=64 #Dim of the final images\n",
    "    nuclear_channel=\"Ch7\"\n",
    "    cellmask_channel=\"Ch1_mask\"\n",
    "    df=pd.read_csv(outpath+\"\\\\cell_info.csv\")\n",
    "    cell_names=df[\"Cell_ID\"].to_numpy()\n",
    "    Prediction_Channels=None\n",
    "\n",
    "    image_dict=load_dict(outpath,cell_names,image_dim)\n",
    "\n",
    "    Channels=['Ch1']  #Channel to be fed to the NN\n",
    "    images_with_index = []\n",
    "\n",
    "    for image_i in image_dict:\n",
    "        #print(image_dict[image_i].keys())\n",
    "        if len(image_dict[image_i].keys())>=len(Channels):\n",
    "            image=cv.merge([image_dict[image_i][i] for i in Channels])\n",
    "            images_with_index.append((int(image_i),image))\n",
    "        else:\n",
    "            print(image_i)\n",
    "        \n",
    "    images=np.array([image[1] for image in images_with_index])\n",
    "    names=np.array([image[0] for image in images_with_index])\n",
    "    assert sum(names!=df['Cell_ID'].to_numpy()) ==0  #Check that the order has been preserved\n",
    "    DNA_pos=df[\"DNA_pos\"].to_numpy()\n",
    "    Touches_Boundary=df[\"Touches_boundary\"].to_numpy()\n",
    "    labels=np.array([N for i in names]) #Labels are the tissues\n",
    "\n",
    "    Thresh=50\n",
    "    plt.hist(df[\"Gradient RMS_M01_Ch01\"],bins=200);\n",
    "    plt.axvline(x=Thresh,color=\"red\")\n",
    "    plt.show()\n",
    "    idx_to_keep=np.array(Touches_Boundary==0,dtype=int)+np.array(df[\"Gradient RMS_M01_Ch01\"]>Thresh,dtype=int)==2 #np.array(DNA_pos==1,dtype=int)+\n",
    "    #Filter\n",
    "    print(len(images))\n",
    "    images=images[idx_to_keep]\n",
    "    names=names[idx_to_keep]\n",
    "    labels=labels[idx_to_keep]\n",
    "\n",
    "    images=images[:cutoff]\n",
    "    names=names[:cutoff]\n",
    "    labels=labels[:cutoff]\n",
    "\n",
    "\n",
    "    mini=int(round(abs(np.array(images).min()),0))\n",
    "    images=images+abs(np.array(images).min())\n",
    "    mean=np.array(images).mean()\n",
    "    maxi=np.array(images).max()\n",
    "    std=np.array(images).std()\n",
    "\n",
    "    print(len(images))\n",
    "\n",
    "\n",
    "    returned=data_generator(images,labels,names,mini,train_test_split = 0.8,batch_size = 100,sample=False)\n",
    "    train,test,batch_size,mean_loader,std_loader=returned\n",
    "    [train_data,train_data1,train_labels,train_ID]=train\n",
    "    [test_data,test_data1,test_labels,test_ID]=test\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    rotated = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.RandomRotation(degrees=18,fill=mini),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[mean_loader], std=[std_loader]),  # for grayscale images\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomVerticalFlip(p=0.5),\n",
    "        transforms.Lambda(lambda x: x if 0.5>np.random.rand() else transforms.functional.invert(x)-1),\n",
    "        transforms.Lambda(lambda x: x *(1+0.2*np.random.randn()) ),\n",
    "    # transforms.Lambda(lambda x: x + (0.1**0.5)*torch.randn(64, 64) )\n",
    "        ]) \n",
    "\n",
    "    transform_test = transforms.Compose(\n",
    "        [transforms.ToPILImage(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[mean_loader], std=[std_loader])\n",
    "         ])\n",
    "\n",
    "\n",
    "\n",
    "    train_data = CellDataset(train_data1,train_labels,train_ID, rotated)\n",
    "    train_data_list.append(train_data)\n",
    "\n",
    "    test_data = CellDataset(test_data1,test_labels,test_ID, transforms=transform_test)\n",
    "    test_data_list.append(test_data)\n",
    "\n",
    "\n",
    "\n",
    "from torch.utils.data import ConcatDataset\n",
    "train_cat = ConcatDataset([train_data for train_data in train_data_list])\n",
    "train_cat_loader = DataLoader(train_cat, batch_size=524,shuffle=True,drop_last=True)\n",
    "\n",
    "\n",
    "test_cat = ConcatDataset([test_data for test_data in test_data_list])\n",
    "test_cat_loader = DataLoader(test_cat, batch_size=524,shuffle=True)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import dill\n",
    "\n",
    "#torch.save(train_cat_loader,'train_data.pth',pickle_module=dill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.getsizeof(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_imgs=next(iter(train_cat_loader))\n",
    "plt.imshow(my_imgs[0][0][2][0],vmin=-10,vmax=10)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.show()\n",
    "plt.imshow(my_imgs[0][1][2][0],vmin=-10,vmax=10)\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "my_imgs=next(iter(test_cat_loader))\n",
    "plt.imshow(my_imgs[0][0][2][0],vmin=-10,vmax=10)\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(my_imgs[0][1][2][0],vmin=-10,vmax=10)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_net=torch.load(r'C:\\Users\\Thibaut Goldsborough\\Documents\\Seth_BoneMarrow\\Results\\\\'+\"model300loss_tensor(93.1900).pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = train_cat_loader\n",
    "test_loader = test_cat_loader\n",
    "\n",
    "def get_val():\n",
    "    total_loss = 0\n",
    "    model.eval()\n",
    "    for (x0, x1), _ in test_loader:\n",
    "        x0 = x0.to(device)\n",
    "        x1 = x1.to(device)\n",
    "        z0 = model(x0)\n",
    "        z1 = model(x1)\n",
    "        loss = criterion(z0, z1)\n",
    "        total_loss += loss.detach()\n",
    "\n",
    "    print(\"Validation Loss:\",int(total_loss / len(test_loader)))\n",
    "    return int(total_loss / len(test_loader))\n",
    "    \n",
    "def train():\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "    for (x0, x1), _ in train_loader:\n",
    "        x0 = x0.to(device)\n",
    "        x1 = x1.to(device)\n",
    "        z0 = model(x0)\n",
    "        z1 = model(x1)\n",
    "        loss = criterion(z0, z1)\n",
    "        total_loss += loss.detach()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "#resnet = resnet18(num_classes=latent_dim,channel_num=len(Channels))\n",
    "#backbone = resnet\n",
    "#model = BarlowTwins(backbone,latent_dim)\n",
    "\n",
    "model=Train_net\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "collate_fn = ImageCollateFunction(input_size=32)\n",
    "\n",
    "criterion = BarlowTwinsLoss(device=device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.06)\n",
    "\n",
    "print(\"Starting Training\")\n",
    "\n",
    "train_losses=[]\n",
    "val_losses=[]\n",
    "for epoch in range(epochs):\n",
    "    avg_loss=train()\n",
    "    print(f\"epoch: {epoch:>02}, loss: {avg_loss:.5f}\")\n",
    "    train_losses.append(avg_loss)\n",
    "    val_losses.append(get_val())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses=[int(i.detach().cpu()) for i in train_losses]\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(train_losses)\n",
    "plt.plot(val_losses)\n",
    "plt.savefig(basepath+\"/Results/Loss\"+str(train_losses[-1])+\".png\",bbox_inches=\"tight\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_cat, batch_size=batch_size,shuffle=True,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_data = CellDataset(train_data1,train_labels,train_ID,rotated)\n",
    "#train_loader = DataLoader(train_data, batch_size=124, shuffle=True)#,sampler=sampler)\n",
    "\n",
    "\n",
    "latent_dim=2048\n",
    "\n",
    "\n",
    "barlow=np.zeros((len(train_loader)*batch_size,latent_dim))\n",
    "names=np.zeros((len(train_loader)*batch_size))\n",
    "labels=np.zeros((len(train_loader)*batch_size))\n",
    "i=0\n",
    "for (x0, x1), Y in train_loader:\n",
    "    x0 = x0.to(device)\n",
    "    x1 = x1.to(device)\n",
    "    latents=model(x0).detach().cpu().numpy()  #model.backbone(x0).detach().cpu().numpy()\n",
    "    names[i:i+len(latents)]=Y[:,1]\n",
    "    labels[i:i+len(latents)]=Y[:,0]\n",
    "    barlow[i:i+len(latents)]=latents\n",
    "    i+=len(latents)\n",
    "    print(i,len(train_data1))\n",
    "\n",
    "df3=pd.DataFrame(barlow)\n",
    "df3[\"Cell_ID\"]=names\n",
    "df3[\"Dataset\"]=[DATASETS[int(i)] for i in labels]\n",
    "#df3.to_csv(basepath+\"/Results/Barlow_latents\"+str(train_losses[-1])+\".csv\", index = False, header=True)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df3\n",
    "cell_names=df[\"Cell_ID\"].to_numpy()\n",
    "Labels=df[\"Dataset\"].to_numpy()\n",
    "\n",
    "latents=df.iloc[:,:].drop([\"Cell_ID\",\"Dataset\"],axis=1).to_numpy()\n",
    "print(latents.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLOR_DICT={\"BoneMarrow_sample1\":\"red\",\"Retina_1_2\":\"limegreen\",\"Colon\":\"magenta\",\"Choroid\":\"blue\"}\n",
    "inv_map = {v: k for k, v in COLOR_DICT.items()}\n",
    "colors=[COLOR_DICT[i] for i in Labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "fit = TSNE(n_components=2)\n",
    "u = fit.fit_transform(latents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as mpatches\n",
    "\n",
    "fig=plt.figure(figsize=(15,10))\n",
    "\n",
    "patches=[mpatches.Patch(color=color, label=tissue) for tissue,color in COLOR_DICT.items()]\n",
    "plt.legend(handles=patches)\n",
    "plt.scatter(u[:,0],u[:,1],c=colors,s=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"U0\"]=u[:,0]\n",
    "df[\"U1\"]=u[:,1]\n",
    "df.to_csv(basepath+df_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "96e92283253362575e1b2577a58171a1def071c2d4840c376515c402fb1735d8"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('deep_learning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
