{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "6074eb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import required packages\n",
    "import cv2 as cv\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader,random_split\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "6a4256fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Insert filepath for local files  FOR THIBAUT\n",
    "basepath = r\"C:\\Users\\Thibaut Goldsborough\\Documents\\Seth_BoneMarrow\\Data\\BoneMarrow_smallerfile2\"\n",
    "readpath = basepath + \"\\\\Raw_Images\"\n",
    "outpath = basepath + \"\\\\Outputs\"\n",
    "file_prefix=\"\\\\smaller_file2_\"\n",
    "\n",
    "num_images=10000\n",
    "\n",
    "image_dim=64 #Dim of the final images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "2324aa3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(outpath+\"\\\\cell_info.csv\")\n",
    "cell_names=np.load(outpath+\"\\\\image_ID.npy\")\n",
    "cell_names=[int(cell_name) for cell_name in cell_names]\n",
    "\n",
    "if sum(df[\"Cell_ID\"].to_numpy()!=cell_names)!=0:\n",
    "    print(\"Error, dataframe cell ID do not match with entries saved during image processing step\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "a1e258b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channels found: ['Ch1', 'Ch11', 'Ch11_mask', 'Ch1_mask', 'Ch2', 'Ch2_mask', 'Ch3', 'Ch3_mask', 'Ch6', 'Ch6_mask', 'Ch7', 'Ch7_mask', 'Ch9', 'Ch9_mask']\n"
     ]
    }
   ],
   "source": [
    "image_dict={}\n",
    "\n",
    "for cell_name in cell_names:\n",
    "    image_dict[cell_name]={}\n",
    "\n",
    "\n",
    "#Find Channels\n",
    "names=[]\n",
    "for entry in os.listdir(outpath): #Read all files\n",
    "    if os.path.isfile(os.path.join(outpath, entry)):\n",
    "        if entry!='image_ID.npy':\n",
    "            names.append(entry)\n",
    "\n",
    "\n",
    "channels=[name[:-4] for name in names if name[-4:]=='.npy']\n",
    "\n",
    "print(\"Channels found:\",channels)\n",
    "\n",
    "data_dict={}\n",
    "for channel in channels:\n",
    "    data_dict[channel]=np.load(outpath+\"\\\\\"+channel+'.npy')\n",
    "\n",
    "#Break up array\n",
    "\n",
    "for channel in data_dict:\n",
    "    dims=data_dict[channel].shape\n",
    "    n=dims[0]//image_dim\n",
    "    l=dims[1]//image_dim\n",
    "    index=0\n",
    "    for i in range(n):\n",
    "        for j in range(l):\n",
    "            img=data_dict[channel][i*image_dim:i*image_dim+image_dim,j*image_dim:j*image_dim+image_dim]\n",
    "            image_dict[cell_names[index]][channel]=img\n",
    "            index+=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "c5715850",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_onehot(my_list):\n",
    "    return_list=[]\n",
    "    for i,elem in enumerate(my_list):\n",
    "        j=np.where(np.unique(labels)==elem)\n",
    "        return_list.append(np.zeros((len(np.unique(my_list)))))\n",
    "        return_list[-1][j]=1\n",
    "    return np.array(return_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "ac96b805",
   "metadata": {},
   "outputs": [],
   "source": [
    "Channels=['Ch1']  #Channel to be fed to the NN\n",
    "\n",
    "# Slightly overkill method to make absolutely sure the images are in order\n",
    "images_with_index = []\n",
    "for image_i in image_dict:\n",
    "    image=cv.merge([image_dict[image_i][i] for i in Channels])\n",
    "    images_with_index.append((int(image_i),image))\n",
    "    \n",
    "images_with_index.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "2ff8f6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "images=[image[1] for image in images_with_index]\n",
    "names=[image[0] for image in images_with_index]\n",
    "names_arg=np.argsort([image[0] for image in images_with_index])\n",
    "labels=df['Cell_Type'].to_numpy()\n",
    "labels=to_onehot(labels[names_arg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "2d8af17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=df['Cell_Type'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "88f9933a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100.0"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.01*10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "87bb6615",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(labels==3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "66e7e6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mini=int(round(abs(np.array(images).min()),0))\n",
    "images=images+abs(np.array(images).min())\n",
    "mean=np.array(images).mean()\n",
    "maxi=np.array(images).max()\n",
    "std=np.array(images).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "3e0fb898",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x146a7a46040>"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(images[0])\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "ed4f3e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split data into training and validation, just looking at first images now\n",
    "\n",
    "validation_split = 0.8\n",
    "test_split = 0.9\n",
    "\n",
    "train_end=int(validation_split*len(images))\n",
    "val_end=int(test_split*len(images))\n",
    "\n",
    "train_data1=images[:train_end]\n",
    "validation_data1=images[train_end:val_end]\n",
    "test_data1=images[val_end:]\n",
    "\n",
    "train_labels=labels[:train_end]\n",
    "validation_labels=labels[train_end:val_end]\n",
    "test_labels=labels[val_end:]\n",
    "\n",
    "train_ID=names[:train_end]\n",
    "validation_ID=names[train_end:val_end]\n",
    "test_ID=names[val_end:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "73df5cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform data into tensors, normalize images\n",
    "transform_basic = transforms.Compose(\n",
    "    [transforms.ToPILImage(),\n",
    "     transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# custom dataset\n",
    "class CellDataset():\n",
    "    def __init__(self, images,labels, transforms=None):\n",
    "        self.X = images\n",
    "        self.Y=  labels\n",
    "        self.transforms = transforms\n",
    "         \n",
    "    def __len__(self):\n",
    "        return (len(self.X))\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        data = self.X[i]\n",
    "        label=self.Y[i]\n",
    "        data = np.asarray(data).astype(np.uint8)\n",
    "        \n",
    "        if self.transforms:\n",
    "            data = self.transforms(data)\n",
    "        \n",
    "        return data,label\n",
    "\n",
    "\n",
    "train_data_basic = CellDataset(train_data1,train_labels, transform_basic)\n",
    "#Create DataLoaders\n",
    "train_loader_basic = DataLoader(train_data_basic, batch_size=100, shuffle=False)\n",
    "\n",
    "#data=next(iter(train_loader_basic))[0] Don't delete this is useful\n",
    "\n",
    "\n",
    "def get_mean_std(loader):\n",
    "    #https://stackoverflow.com/questions/48818619/pytorch-how-do-the-means-and-stds-get-calculated-in-the-transfer-learning-tutor\n",
    "    mean = 0.\n",
    "    std = 0.\n",
    "    for images, _ in loader:\n",
    "        batch_samples = images.size(0) # batch size (the last batch can have smaller size!)\n",
    "        images = images.view(batch_samples, images.size(1), -1)\n",
    "        mean += images.mean(2).sum(0)\n",
    "        std += images.std(2).sum(0)\n",
    "\n",
    "    mean /= len(loader.dataset)\n",
    "    std /= len(loader.dataset)\n",
    "    return mean, std\n",
    "\n",
    "\n",
    "mean_loader,std_loader=get_mean_std(train_loader_basic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "ec262f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Free up Space  #Optional\n",
    "# import gc\n",
    "\n",
    "# del images\n",
    "# del shuffle_list\n",
    "# del data_dict\n",
    "# del images_with_index\n",
    "\n",
    "del train_data_basic\n",
    "del train_loader_basic\n",
    "\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "3857d8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def polar_transform(images, transform_type='linearpolar'):\n",
    "    images=np.array(images).reshape((len(images),1,60,60))\n",
    "    \"\"\"\n",
    "    This function takes multiple images, and apply polar coordinate conversion to it.\n",
    "    \"\"\"\n",
    "    \n",
    "    (N, C, H, W) = images.shape\n",
    "\n",
    "    for i in range(images.shape[0]):\n",
    "\n",
    "        img = images[i]  # [C,H,W]\n",
    "        img = np.transpose(img, (1, 2, 0))  # [H,W,C]\n",
    "\n",
    "        if transform_type == 'logpolar':\n",
    "            img = cv.logPolar(img, (H // 2, W // 2), W / math.log(W / 2), cv.WARP_FILL_OUTLIERS).reshape(H, W, C)\n",
    "        elif transform_type == 'linearpolar':\n",
    "            img = cv.linearPolar(img, (H // 2, W // 2), W / 2, cv.WARP_FILL_OUTLIERS).reshape(H, W, C)\n",
    "        img = np.transpose(img, (2, 0, 1))\n",
    "\n",
    "        images[i] = torch.from_numpy(img)\n",
    "\n",
    "    return images.reshape((len(images),60,60))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "99ebb947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "#Get information about CPU/GPU\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        device = 'cuda:0'\n",
    "    else:\n",
    "        device = 'cpu'\n",
    "    return device\n",
    "device = get_device()\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "be3aec27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(image,noise_factor):\n",
    "    return image+ noise_factor * torch.randn(*image.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "ca8365c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Type, Any, Callable, Union, List, Optional\n",
    "from torch import Tensor\n",
    "\n",
    "#from .._internally_replaced_utils import load_state_dict_from_url\n",
    "#from ..utils import _log_api_usage_once\n",
    "\n",
    "\n",
    "\n",
    "def conv3x3(in_planes: int, out_planes: int, stride: int = 1, groups: int = 1, dilation: int = 1) -> nn.Conv2d:\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(\n",
    "        in_planes,\n",
    "        out_planes,\n",
    "        kernel_size=3,\n",
    "        stride=stride,\n",
    "        padding=dilation,\n",
    "        groups=groups,\n",
    "        bias=False,\n",
    "        dilation=dilation,\n",
    "    )\n",
    "\n",
    "\n",
    "def conv1x1(in_planes: int, out_planes: int, stride: int = 1) -> nn.Conv2d:\n",
    "    \"\"\"1x1 convolution\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion: int = 1\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        inplanes: int,\n",
    "        planes: int,\n",
    "        stride: int = 1,\n",
    "        downsample: Optional[nn.Module] = None,\n",
    "        groups: int = 1,\n",
    "        base_width: int = 64,\n",
    "        dilation: int = 1,\n",
    "        norm_layer: Optional[Callable[..., nn.Module]] = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        if groups != 1 or base_width != 64:\n",
    "            raise ValueError(\"BasicBlock only supports groups=1 and base_width=64\")\n",
    "        if dilation > 1:\n",
    "            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
    "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = norm_layer(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = norm_layer(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    # Bottleneck in torchvision places the stride for downsampling at 3x3 convolution(self.conv2)\n",
    "    # while original implementation places the stride at the first 1x1 convolution(self.conv1)\n",
    "    # according to \"Deep residual learning for image recognition\"https://arxiv.org/abs/1512.03385.\n",
    "    # This variant is also known as ResNet V1.5 and improves accuracy according to\n",
    "    # https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch.\n",
    "\n",
    "    expansion: int = 4\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        inplanes: int,\n",
    "        planes: int,\n",
    "        stride: int = 1,\n",
    "        downsample: Optional[nn.Module] = None,\n",
    "        groups: int = 1,\n",
    "        base_width: int = 64,\n",
    "        dilation: int = 1,\n",
    "        norm_layer: Optional[Callable[..., nn.Module]] = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        width = int(planes * (base_width / 64.0)) * groups\n",
    "        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv1 = conv1x1(inplanes, width)\n",
    "        self.bn1 = norm_layer(width)\n",
    "        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n",
    "        self.bn2 = norm_layer(width)\n",
    "        self.conv3 = conv1x1(width, planes * self.expansion)\n",
    "        self.bn3 = norm_layer(planes * self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        block: Type[Union[BasicBlock, Bottleneck]],\n",
    "        layers: List[int],\n",
    "        num_classes: int = 4,\n",
    "        zero_init_residual: bool = False,\n",
    "        groups: int = 1,\n",
    "        width_per_group: int = 64,\n",
    "        replace_stride_with_dilation: Optional[List[bool]] = None,\n",
    "        norm_layer: Optional[Callable[..., nn.Module]] = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        #_log_api_usage_once(self)\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        self._norm_layer = norm_layer\n",
    "\n",
    "        self.inplanes = 64\n",
    "        self.dilation = 1\n",
    "        if replace_stride_with_dilation is None:\n",
    "            # each element in the tuple indicates if we should replace\n",
    "            # the 2x2 stride with a dilated convolution instead\n",
    "            replace_stride_with_dilation = [False, False, False]\n",
    "        if len(replace_stride_with_dilation) != 3:\n",
    "            raise ValueError(\n",
    "                \"replace_stride_with_dilation should be None \"\n",
    "                f\"or a 3-element tuple, got {replace_stride_with_dilation}\"\n",
    "            )\n",
    "        self.groups = groups\n",
    "        self.base_width = width_per_group\n",
    "        self.conv1 = nn.Conv2d(len(Channels), self.inplanes, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = norm_layer(self.inplanes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2, dilate=replace_stride_with_dilation[0])\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2, dilate=replace_stride_with_dilation[1])\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2, dilate=replace_stride_with_dilation[2])\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        # Zero-initialize the last BN in each residual branch,\n",
    "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
    "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
    "        if zero_init_residual:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, Bottleneck):\n",
    "                    nn.init.constant_(m.bn3.weight, 0)  # type: ignore[arg-type]\n",
    "                elif isinstance(m, BasicBlock):\n",
    "                    nn.init.constant_(m.bn2.weight, 0)  # type: ignore[arg-type]\n",
    "\n",
    "    def _make_layer(\n",
    "        self,\n",
    "        block: Type[Union[BasicBlock, Bottleneck]],\n",
    "        planes: int,\n",
    "        blocks: int,\n",
    "        stride: int = 1,\n",
    "        dilate: bool = False,\n",
    "    ) -> nn.Sequential:\n",
    "        norm_layer = self._norm_layer\n",
    "        downsample = None\n",
    "        previous_dilation = self.dilation\n",
    "        if dilate:\n",
    "            self.dilation *= stride\n",
    "            stride = 1\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
    "                norm_layer(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(\n",
    "            block(\n",
    "                self.inplanes, planes, stride, downsample, self.groups, self.base_width, previous_dilation, norm_layer\n",
    "            )\n",
    "        )\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(\n",
    "                block(\n",
    "                    self.inplanes,\n",
    "                    planes,\n",
    "                    groups=self.groups,\n",
    "                    base_width=self.base_width,\n",
    "                    dilation=self.dilation,\n",
    "                    norm_layer=norm_layer,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _forward_impl(self, x: Tensor) -> Tensor:\n",
    "        # See note [TorchScript super()]\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self._forward_impl(x)\n",
    "\n",
    "\n",
    "\n",
    "def _resnet(\n",
    "    arch: str,\n",
    "    block: Type[Union[BasicBlock, Bottleneck]],\n",
    "    layers: List[int],\n",
    "    pretrained: bool,\n",
    "    progress: bool,\n",
    "    **kwargs: Any,\n",
    ") -> ResNet:\n",
    "    model = ResNet(block, layers, **kwargs)\n",
    "    if pretrained:\n",
    "        state_dict = load_state_dict_from_url(model_urls[arch], progress=progress)\n",
    "        model.load_state_dict(state_dict)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "19e7b5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train and evaluate model\n",
    "### Training function\n",
    "def train_epoch(NN, device, dataloader, loss_fn, optimizer,noise_factor=0):\n",
    "    # Set train mode for both the encoder and the decoder\n",
    "    NN.train()\n",
    "    train_loss = []\n",
    "    total=0\n",
    "    correct=0\n",
    "    # Iterate the dataloader (we do not need the label values, this is unsupervised learning)\n",
    "    for image_batch,labels_batch in dataloader: # with \"_\" we just ignore the labels (the second element of the dataloader tuple)\n",
    "        image_noisy = add_noise(image_batch,noise_factor) \n",
    "        image_batch = image_noisy.to(device)\n",
    "        labels_batch=labels_batch.to(device)\n",
    "        output = NN(image_batch)\n",
    "        # Evaluate loss\n",
    "        loss = loss_fn(output,labels_batch)\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss.append(loss.detach().cpu().numpy())\n",
    "\n",
    "        total+=labels_batch.size(0)\n",
    "\n",
    "        output=output.detach().cpu().numpy()\n",
    "        labels=labels_batch.detach().cpu().numpy()\n",
    "\n",
    "        correct+=np.sum(np.argmax(output,axis=1)==np.argmax(labels,axis=1))\n",
    "\n",
    "\n",
    "    accuracy=100 * correct / total\n",
    "\n",
    "    return np.mean(train_loss),accuracy\n",
    "\n",
    "\n",
    "### validationing function\n",
    "def validation_epoch(NN, device, dataloader, loss_fn):\n",
    "    # Set evaluation mode for encoder and decoder\n",
    "    NN.eval()\n",
    "    total=0\n",
    "    correct=0\n",
    "    val_loss=[]\n",
    "\n",
    "    with torch.no_grad(): # No need to track the gradients\n",
    "\n",
    "        for image_batch,labels_batch in dataloader:\n",
    "            # Move tensor to the proper device\n",
    "            image_batch = image_batch.to(device)\n",
    "            labels_batch = labels_batch.to(device)\n",
    "            output = NN(image_batch)\n",
    "            loss = loss_fn(output,labels_batch)\n",
    "\n",
    "            total+=labels_batch.size(0)\n",
    "\n",
    "            output=output.detach().cpu().numpy()\n",
    "            labels=labels_batch.detach().cpu().numpy()\n",
    "\n",
    "            correct+=np.sum(np.argmax(output,axis=1)==np.argmax(labels,axis=1))\n",
    "\n",
    "            val_loss.append(loss.detach().cpu().numpy())\n",
    "\n",
    "    accuracy=100 * correct / total\n",
    "    return np.mean(val_loss),accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "f26ba1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot losses at the end of training\n",
    "def loss_over_epochs(diz_loss,num_epochs):\n",
    "    plt.figure(figsize=(10,8))\n",
    "    plt.semilogy(diz_loss['train_loss'], label='Train')\n",
    "    plt.semilogy(diz_loss['val_loss'], label='Valid')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Average Loss')\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    plt.title('Loss over ' + str(num_epochs) + ' epochs')\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(10,8))\n",
    "    plt.plot(diz_loss['train_acc'], label='Train')\n",
    "    plt.plot(diz_loss['val_acc'], label='Valid')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Average Accuracy')\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    plt.title('Loss over ' + str(num_epochs) + ' epochs')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8358998",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "98d2fbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py\n",
    "\n",
    "\n",
    "def resnet18(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet:\n",
    "    return _resnet(\"resnet18\", BasicBlock, [2, 2, 2, 2], pretrained, progress, **kwargs)\n",
    "\n",
    "\n",
    "def resnet34(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet:\n",
    "    r\"\"\"ResNet-34 model from\n",
    "    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    return _resnet(\"resnet34\", BasicBlock, [3, 4, 6, 3], pretrained, progress, **kwargs)\n",
    "\n",
    "\n",
    "def resnet50(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet:\n",
    "    r\"\"\"ResNet-50 model from\n",
    "    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    return _resnet(\"resnet50\", Bottleneck, [3, 4, 6, 3], pretrained, progress, **kwargs)\n",
    "\n",
    "\n",
    "def resnet101(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet:\n",
    "    r\"\"\"ResNet-101 model from\n",
    "    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    return _resnet(\"resnet101\", Bottleneck, [3, 4, 23, 3], pretrained, progress, **kwargs)\n",
    "\n",
    "\n",
    "def resnet152(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet:\n",
    "    r\"\"\"ResNet-152 model from\n",
    "    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    return _resnet(\"resnet152\", Bottleneck, [3, 8, 36, 3], pretrained, progress, **kwargs)\n",
    "\n",
    "\n",
    "def resnext50_32x4d(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet:\n",
    "    r\"\"\"ResNeXt-50 32x4d model from\n",
    "    `\"Aggregated Residual Transformation for Deep Neural Networks\" <https://arxiv.org/pdf/1611.05431.pdf>`_.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    kwargs[\"groups\"] = 32\n",
    "    kwargs[\"width_per_group\"] = 4\n",
    "    return _resnet(\"resnext50_32x4d\", Bottleneck, [3, 4, 6, 3], pretrained, progress, **kwargs)\n",
    "\n",
    "\n",
    "def resnext101_32x8d(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet:\n",
    "    r\"\"\"ResNeXt-101 32x8d model from\n",
    "    `\"Aggregated Residual Transformation for Deep Neural Networks\" <https://arxiv.org/pdf/1611.05431.pdf>`_.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    kwargs[\"groups\"] = 32\n",
    "    kwargs[\"width_per_group\"] = 8\n",
    "    return _resnet(\"resnext101_32x8d\", Bottleneck, [3, 4, 23, 3], pretrained, progress, **kwargs)\n",
    "\n",
    "\n",
    "def wide_resnet50_2(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet:\n",
    "    r\"\"\"Wide ResNet-50-2 model from\n",
    "    `\"Wide Residual Networks\" <https://arxiv.org/pdf/1605.07146.pdf>`_.\n",
    "    The model is the same as ResNet except for the bottleneck number of channels\n",
    "    which is twice larger in every block. The number of channels in outer 1x1\n",
    "    convolutions is the same, e.g. last block in ResNet-50 has 2048-512-2048\n",
    "    channels, and in Wide ResNet-50-2 has 2048-1024-2048.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    kwargs[\"width_per_group\"] = 64 * 2\n",
    "    return _resnet(\"wide_resnet50_2\", Bottleneck, [3, 4, 6, 3], pretrained, progress, **kwargs)\n",
    "\n",
    "\n",
    "def wide_resnet101_2(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet:\n",
    "    r\"\"\"Wide ResNet-101-2 model from\n",
    "    `\"Wide Residual Networks\" <https://arxiv.org/pdf/1605.07146.pdf>`_.\n",
    "    The model is the same as ResNet except for the bottleneck number of channels\n",
    "    which is twice larger in every block. The number of channels in outer 1x1\n",
    "    convolutions is the same, e.g. last block in ResNet-50 has 2048-512-2048\n",
    "    channels, and in Wide ResNet-50-2 has 2048-1024-2048.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    kwargs[\"width_per_group\"] = 64 * 2\n",
    "    return _resnet(\"wide_resnet101_2\", Bottleneck, [3, 4, 23, 3], pretrained, progress, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "70c9da50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Run_NN(train_data=train_data1,validation_data=validation_data1,model=resnet18(False,True),str_model='resnet18',batch_size=20,num_epochs = 25,polar=False,invert=False,lr=0.001):\n",
    "\n",
    "        #Oversampling\n",
    "    #Transform data into tensors, normalize images\n",
    "    transform_train = transforms.Compose(\n",
    "        [transforms.ToPILImage(),transforms.RandomHorizontalFlip(p=0.5),transforms.RandomVerticalFlip(p=0.5),transforms.RandomRotation(degrees=180,fill=mini),\n",
    "       # transforms.GaussianBlur(kernel_size=(3, 3),sigma=(0.5,0.5)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean_loader,std_loader)  \n",
    "    ])\n",
    "\n",
    "    #Transform data into tensors, normalize images\n",
    "    transform_validation = transforms.Compose(\n",
    "        [transforms.ToPILImage(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean_loader,std_loader)  \n",
    "    ])\n",
    "\n",
    "    from torch.utils.data.sampler import WeightedRandomSampler\n",
    "    counts=np.bincount(np.argmax(train_labels,axis=1))\n",
    "    labels_weights = 1. / counts\n",
    "    weights = labels_weights[np.argmax(train_labels,axis=1)]\n",
    "    sampler = WeightedRandomSampler(weights, len(weights))\n",
    "\n",
    "    if invert:\n",
    "        transform_train = transforms.Compose(\n",
    "        [transforms.ToPILImage(),transforms.RandomHorizontalFlip(p=0.5),transforms.RandomVerticalFlip(p=0.5),transforms.RandomRotation(degrees=180,fill=mini),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean_loader,std_loader),\n",
    "        torchvision.transforms.RandomInvert(p=0.5) \n",
    "        ])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    if polar:\n",
    "        transform_train = transforms.Compose(\n",
    "        [transforms.ToPILImage(),\n",
    "        transforms.GaussianBlur(kernel_size=(3, 3),sigma=(0.5,0.5)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean_loader,std_loader)  \n",
    "    ])\n",
    "\n",
    "        train_data=polar_transform(train_data,'logpolar')\n",
    "        validation_data=polar_transform(validation_data,'logpolar')\n",
    "\n",
    "\n",
    "    train_data = CellDataset(train_data,train_labels, transform_train)\n",
    "    validation_data = CellDataset(validation_data,validation_labels, transform_validation)\n",
    "\n",
    "\n",
    "\n",
    "    #Create DataLoaders\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=False,sampler=sampler)\n",
    "    validation_loader = DataLoader(validation_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    ConvNet_simple=model\n",
    "    ConvNet_simple.to(device)\n",
    "    loss_fn = nn.CrossEntropyLoss()   \n",
    "    optimizer = optim.Adam(ConvNet_simple.parameters(), lr = lr) \n",
    "    \n",
    "    diz_loss = {'train_loss':[],'val_loss':[],'train_acc':[],'val_acc':[]}\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        train_loss,train_acc = train_epoch(ConvNet_simple,device,train_loader,loss_fn,optimizer)\n",
    "        val_loss,val_acc = validation_epoch(ConvNet_simple,device,validation_loader,loss_fn)\n",
    "        \n",
    "        print('\\n EPOCH',epoch+1,' \\t train loss',train_loss,' \\t val loss',val_loss,'\\t train acc',train_acc,'\\t val acc',val_acc)\n",
    "        diz_loss['train_loss'].append(train_loss)\n",
    "        diz_loss['val_loss'].append(val_loss)\n",
    "        diz_loss['train_acc'].append(train_acc)\n",
    "        diz_loss['val_acc'].append(val_acc)\n",
    "\n",
    "    _ = loss_over_epochs(diz_loss,num_epochs)\n",
    "\n",
    "    return diz_loss,str_model,ConvNet_simple,validation_loader,train_loader\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "2586c77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preds(NN,validation_loader):\n",
    "    y_trues=[]\n",
    "    y_preds=[]\n",
    "\n",
    "\n",
    "    #validation_loader = DataLoader(validation_data, batch_size=int(len(validation_data)/10), shuffle=False)\n",
    "\n",
    "    with torch.no_grad(): # No need to track the gradients\n",
    "\n",
    "        for image_batch,labels_batch in validation_loader:\n",
    "            # Move tensor to the proper device\n",
    "            image_batch = image_batch.to(device)\n",
    "\n",
    "            labels=labels_batch.to(device).detach().cpu().numpy()\n",
    "            y_true=np.argmax(labels,axis=1)\n",
    "\n",
    "            output=NN(image_batch).detach().cpu().numpy()\n",
    "            y_pred=np.argmax(output,axis=1)\n",
    "\n",
    "            y_trues.extend(list(y_true))\n",
    "            y_preds.extend(list(y_pred))     \n",
    "\n",
    "\n",
    "\n",
    "    conf=confusion_matrix(y_trues, y_preds,labels=[0,1,2])\n",
    "\n",
    "    return y_trues,y_preds,conf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "e9fe8714",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "#from plotcm import plot_confusion_matrix\n",
    "#plt.figure(figsize=(5,5),dpi=100)\n",
    "##plot_confusion_matrix(conf, [\"Singlet\",\"Doublet\",\"Debris\"],normalize=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "e3281a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the model outputs\n",
    "\n",
    "def save_output(diz_loss,str_model,y_trues,y_preds,conf):\n",
    "    name=str_model\n",
    "    mean_acc=round(np.mean(diz_loss['val_acc'][-5:]),2)\n",
    "    new_dir=basepath+\"\\\\Results\\\\\"+name+'_'+str(mean_acc)\n",
    "\n",
    "    dir=new_dir\n",
    "    count=1\n",
    "    while os.path.exists(dir):\n",
    "        dir=new_dir+'('+str(count)+')'\n",
    "        count+=1\n",
    "\n",
    "    new_dir=dir\n",
    "    os.mkdir(new_dir)\n",
    "\n",
    "    destination1=new_dir+'\\\\Loss.csv'\n",
    "    destination2=new_dir+'\\\\Predictions.csv'\n",
    "    destination3=new_dir+'\\\\Confusion_mat.png'\n",
    "\n",
    "    df = pd.DataFrame.from_dict(diz_loss)\n",
    "    df.to_csv (destination1, index = False, header=True)\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    df['Cell_ID']  = validation_ID\n",
    "    df['Prediction']  = y_preds\n",
    "    df['Ground Truth']=y_trues\n",
    "    df.to_csv (destination2, index = False, header=True)\n",
    "\n",
    "    fig=plt.figure(figsize=(5,5),dpi=150)\n",
    "    plot_confusion_matrix(conf, [\"Singlet\",\"Doublet\",\"Debris\"],normalize=True)\n",
    "    fig.savefig(destination3,bbox_inches='tight', dpi=150)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "04f3b40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# diz_loss,str_model,NN,validation_loader,train_loader=Run_NN(model=resnet18(False,True),str_model='Polar_resnet18',num_epochs=25,polar=True,batch_size=100)\n",
    "# y_trues,y_preds,conf=get_preds(NN,validation_loader)\n",
    "# #save_output(diz_loss,str_model,y_trues,y_preds,conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "9e012f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# diz_loss,str_model,NN,validation_loader,_=Run_NN(model=resnet50(False,True),str_model='Polar_resnet50',num_epochs=100,polar=True,batch_size=20)\n",
    "# y_trues,y_preds,conf=get_preds(NN,validation_loader)\n",
    "# save_output(diz_loss,str_model,y_trues,y_preds,conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "f10780a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# diz_loss,str_model,NN,validation_loader,_=Run_NN(model=resnet152(False,True),str_model='Polar_resnet152',num_epochs=100,polar=True,batch_size=20)\n",
    "# y_trues,y_preds,conf=get_preds(NN,validation_loader)\n",
    "# save_output(diz_loss,str_model,y_trues,y_preds,conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "8de06f19",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\THIBAU~1\\AppData\\Local\\Temp/ipykernel_2960/3242501384.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtrial\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mdiz_loss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstr_model\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mNN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_loader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mRun_NN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresnet18\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstr_model\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Resnet18'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'(trial='\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m')'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpolar\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0my_trues\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_preds\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mget_preds\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0msave_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdiz_loss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstr_model\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_trues\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_preds\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\THIBAU~1\\AppData\\Local\\Temp/ipykernel_2960/502761132.py\u001b[0m in \u001b[0;36mRun_NN\u001b[1;34m(train_data, validation_data, model, str_model, batch_size, num_epochs, polar, invert, lr)\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m         \u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mConvNet_simple\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m         \u001b[0mval_loss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mval_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidation_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mConvNet_simple\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_loader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\THIBAU~1\\AppData\\Local\\Temp/ipykernel_2960/353912202.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[1;34m(NN, device, dataloader, loss_fn, optimizer, noise_factor)\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0mtrain_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0mtotal\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[0mlabels_batch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for trial in range(3):\n",
    "    diz_loss,str_model,NN,validation_loader,train_loader=Run_NN(model=resnet18(),str_model='Resnet18'+'(trial='+str(trial)+')',num_epochs=100,polar=False,batch_size=20,lr=1e-4)\n",
    "    y_trues,y_preds,conf=get_preds(NN,validation_loader)\n",
    "    save_output(diz_loss,str_model,y_trues,y_preds,conf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5f095c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for trial in range(3):\n",
    "    diz_loss,str_model,NN,validation_loader,train_loader=Run_NN(model=resnet50(),str_model='Resnet50'+'(trial='+str(trial)+')',num_epochs=100,polar=False,batch_size=20,lr=1e-4)\n",
    "    y_trues,y_preds,conf=get_preds(NN,validation_loader)\n",
    "    save_output(diz_loss,str_model,y_trues,y_preds,conf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02aa4356",
   "metadata": {},
   "outputs": [],
   "source": [
    "for trial in range(3):\n",
    "    diz_loss,str_model,NN,validation_loader,train_loader=Run_NN(model=resnet34(),str_model='Resnet34'+'(trial='+str(trial)+')',num_epochs=100,polar=False,batch_size=20,lr=1e-4)\n",
    "    y_trues,y_preds,conf=get_preds(NN,validation_loader)\n",
    "    save_output(diz_loss,str_model,y_trues,y_preds,conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd1cde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=0/0 #Break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f00a2c5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\THIBAU~1\\AppData\\Local\\Temp/ipykernel_2960/3961005993.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'train_loader' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "05ba9d69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " EPOCH 1  \t train loss 1.3009307883414443  \t val loss 1.5962609926760194 \t train acc 37.5 \t val acc 16.5\n",
      "\n",
      " EPOCH 2  \t train loss 1.04273378543125  \t val loss 1.3298114000856875 \t train acc 52.475 \t val acc 29.2\n",
      "\n",
      " EPOCH 3  \t train loss 0.8763643116667518  \t val loss 1.181955121845007 \t train acc 58.7625 \t val acc 35.3\n",
      "\n",
      " EPOCH 4  \t train loss 0.7582739368305557  \t val loss 1.3535364558696747 \t train acc 63.5125 \t val acc 33.5\n",
      "\n",
      " EPOCH 5  \t train loss 0.6731127119155719  \t val loss 1.1655665108561515 \t train acc 66.95 \t val acc 38.7\n",
      "\n",
      " EPOCH 6  \t train loss 0.612058802130643  \t val loss 1.2280109651237727 \t train acc 68.1875 \t val acc 41.7\n",
      "\n",
      " EPOCH 7  \t train loss 0.572970149413567  \t val loss 0.9989990829527379 \t train acc 69.9 \t val acc 43.0\n",
      "\n",
      " EPOCH 8  \t train loss 0.544271234335336  \t val loss 1.0662465993836525 \t train acc 70.825 \t val acc 42.6\n",
      "\n",
      " EPOCH 9  \t train loss 0.5071245768563413  \t val loss 0.97486496090889 \t train acc 71.45 \t val acc 43.3\n",
      "\n",
      " EPOCH 10  \t train loss 0.4801715205676551  \t val loss 0.8822182057499888 \t train acc 71.8 \t val acc 46.7\n",
      "\n",
      " EPOCH 11  \t train loss 0.4878007506216502  \t val loss 0.8730046310126782 \t train acc 71.1125 \t val acc 48.0\n",
      "\n",
      " EPOCH 12  \t train loss 0.4613189803560717  \t val loss 1.0395646249353885 \t train acc 72.7125 \t val acc 45.3\n",
      "\n",
      " EPOCH 13  \t train loss 0.4585181229776771  \t val loss 0.9140578151643277 \t train acc 71.8625 \t val acc 45.9\n",
      "\n",
      " EPOCH 14  \t train loss 0.43356085925268717  \t val loss 0.8636915751695635 \t train acc 73.8375 \t val acc 47.9\n",
      "\n",
      " EPOCH 15  \t train loss 0.4401390645734352  \t val loss 0.8795658714473247 \t train acc 73.925 \t val acc 47.3\n",
      "\n",
      " EPOCH 16  \t train loss 0.44404081110674426  \t val loss 0.932688182204962 \t train acc 72.8625 \t val acc 48.0\n",
      "\n",
      " EPOCH 17  \t train loss 0.43017897527315824  \t val loss 0.8778105401098729 \t train acc 73.5 \t val acc 46.5\n",
      "\n",
      " EPOCH 18  \t train loss 0.4134499149884674  \t val loss 0.858989483654499 \t train acc 74.0875 \t val acc 47.4\n",
      "\n",
      " EPOCH 19  \t train loss 0.4134162748313126  \t val loss 0.8761629718840124 \t train acc 73.6375 \t val acc 49.8\n",
      "\n",
      " EPOCH 20  \t train loss 0.421289666383761  \t val loss 0.8918279697895051 \t train acc 73.7875 \t val acc 50.0\n",
      "\n",
      " EPOCH 21  \t train loss 0.4117024918453979  \t val loss 0.9092706935107709 \t train acc 74.2625 \t val acc 46.8\n",
      "\n",
      " EPOCH 22  \t train loss 0.4042653937124164  \t val loss 0.8286028849780561 \t train acc 73.8 \t val acc 47.7\n",
      "\n",
      " EPOCH 23  \t train loss 0.403501767615423  \t val loss 0.8782439841628075 \t train acc 74.825 \t val acc 45.7\n",
      "\n",
      " EPOCH 24  \t train loss 0.3917224808132563  \t val loss 0.8340915091335773 \t train acc 75.4 \t val acc 49.2\n",
      "\n",
      " EPOCH 25  \t train loss 0.40395745190070814  \t val loss 0.8055579500198364 \t train acc 74.6875 \t val acc 49.3\n",
      "\n",
      " EPOCH 26  \t train loss 0.39013895851294467  \t val loss 0.8433447733819485 \t train acc 75.15 \t val acc 48.6\n",
      "\n",
      " EPOCH 27  \t train loss 0.3877994505445862  \t val loss 0.8147201181054118 \t train acc 74.8625 \t val acc 47.1\n",
      "\n",
      " EPOCH 28  \t train loss 0.3941199627896959  \t val loss 0.8325868517458439 \t train acc 75.3 \t val acc 47.7\n",
      "\n",
      " EPOCH 29  \t train loss 0.3893603050962923  \t val loss 0.8075983271300793 \t train acc 74.4875 \t val acc 49.8\n",
      "\n",
      " EPOCH 30  \t train loss 0.37777620434210996  \t val loss 0.8322538900971412 \t train acc 75.7625 \t val acc 47.1\n",
      "\n",
      " EPOCH 31  \t train loss 0.3790690076429057  \t val loss 0.8318639652431011 \t train acc 75.8375 \t val acc 50.1\n",
      "\n",
      " EPOCH 32  \t train loss 0.3771070820328232  \t val loss 0.8752561429888011 \t train acc 76.125 \t val acc 48.7\n",
      "\n",
      " EPOCH 33  \t train loss 0.3819463666521077  \t val loss 0.8617706733345984 \t train acc 76.0875 \t val acc 46.9\n",
      "\n",
      " EPOCH 34  \t train loss 0.38351187539725395  \t val loss 0.8281516821980478 \t train acc 75.8625 \t val acc 49.7\n",
      "\n",
      " EPOCH 35  \t train loss 0.37751071626575183  \t val loss 0.8330384381860496 \t train acc 76.3875 \t val acc 48.0\n",
      "\n",
      " EPOCH 36  \t train loss 0.3828009198219172  \t val loss 0.8306200140714646 \t train acc 75.25 \t val acc 48.4\n",
      "\n",
      " EPOCH 37  \t train loss 0.3912721444598046  \t val loss 0.8827730946838857 \t train acc 75.8375 \t val acc 48.7\n",
      "\n",
      " EPOCH 38  \t train loss 0.3667549650097378  \t val loss 0.9047431730851531 \t train acc 76.325 \t val acc 47.7\n",
      "\n",
      " EPOCH 39  \t train loss 0.3729322581919362  \t val loss 0.8437174278497698 \t train acc 75.8375 \t val acc 49.1\n",
      "\n",
      " EPOCH 40  \t train loss 0.37921565256304296  \t val loss 0.8470722726881503 \t train acc 76.125 \t val acc 47.2\n",
      "\n",
      " EPOCH 41  \t train loss 0.3721409132733363  \t val loss 0.8234013347178698 \t train acc 75.75 \t val acc 51.6\n",
      "\n",
      " EPOCH 42  \t train loss 0.363869188947362  \t val loss 0.8126443980634214 \t train acc 76.225 \t val acc 48.3\n",
      "\n",
      " EPOCH 43  \t train loss 0.37182649426952663  \t val loss 0.8024129302203657 \t train acc 76.3375 \t val acc 49.8\n",
      "\n",
      " EPOCH 44  \t train loss 0.38055753123338204  \t val loss 0.8104081713408233 \t train acc 76.025 \t val acc 49.4\n",
      "\n",
      " EPOCH 45  \t train loss 0.366506189882086  \t val loss 0.8268618177920581 \t train acc 76.775 \t val acc 50.7\n",
      "\n",
      " EPOCH 46  \t train loss 0.35911186294502884  \t val loss 0.8335779591798782 \t train acc 77.275 \t val acc 51.4\n",
      "\n",
      " EPOCH 47  \t train loss 0.3638765857520848  \t val loss 0.8349963642433287 \t train acc 76.5125 \t val acc 49.9\n",
      "\n",
      " EPOCH 48  \t train loss 0.35669505608184543  \t val loss 0.8257257808744908 \t train acc 77.425 \t val acc 48.2\n",
      "\n",
      " EPOCH 49  \t train loss 0.36189569201508465  \t val loss 0.9403026833683252 \t train acc 77.925 \t val acc 45.3\n",
      "\n",
      " EPOCH 50  \t train loss 0.363556507706597  \t val loss 0.8415455896630882 \t train acc 77.0875 \t val acc 51.2\n",
      "\n",
      " EPOCH 51  \t train loss 0.35950906264489463  \t val loss 0.815745942361653 \t train acc 78.1125 \t val acc 50.5\n",
      "\n",
      " EPOCH 52  \t train loss 0.3634553954242461  \t val loss 0.8245083980858325 \t train acc 77.1375 \t val acc 51.5\n",
      "\n",
      " EPOCH 53  \t train loss 0.35866966885760904  \t val loss 0.8269049479067325 \t train acc 78.0375 \t val acc 50.8\n",
      "\n",
      " EPOCH 54  \t train loss 0.3526581824518392  \t val loss 0.8228720636963844 \t train acc 77.8 \t val acc 48.2\n",
      "\n",
      " EPOCH 55  \t train loss 0.36539328957393763  \t val loss 0.8022595677673816 \t train acc 77.375 \t val acc 50.9\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\THIBAU~1\\AppData\\Local\\Temp/ipykernel_2960/3307533981.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdiz_loss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstr_model\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mNN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_loader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mRun_NN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresnet18\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstr_model\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Resnet18'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpolar\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0my_trues\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_preds\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mget_preds\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0msave_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdiz_loss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstr_model\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_trues\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_preds\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\THIBAU~1\\AppData\\Local\\Temp/ipykernel_2960/502761132.py\u001b[0m in \u001b[0;36mRun_NN\u001b[1;34m(train_data, validation_data, model, str_model, batch_size, num_epochs, polar, invert, lr)\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m         \u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mConvNet_simple\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m         \u001b[0mval_loss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mval_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidation_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mConvNet_simple\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_loader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\THIBAU~1\\AppData\\Local\\Temp/ipykernel_2960/353912202.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[1;34m(NN, device, dataloader, loss_fn, optimizer, noise_factor)\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0mtrain_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0mtotal\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[0mlabels_batch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "diz_loss,str_model,NN,validation_loader,train_loader=Run_NN(model=resnet18(),str_model='Resnet18',num_epochs=100,polar=False,batch_size=20)\n",
    "y_trues,y_preds,conf=get_preds(NN,validation_loader)\n",
    "save_output(diz_loss,str_model,y_trues,y_preds,conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce86e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "diz_loss,str_model,NN,validation_loader,_=Run_NN(model=resnet50(False,True),str_model='Resnet50_Norm',num_epochs=100,polar=False,batch_size=20)\n",
    "y_trues,y_preds,conf=get_preds(NN,validation_loader)\n",
    "save_output(diz_loss,str_model,y_trues,y_preds,conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7722da51",
   "metadata": {},
   "outputs": [],
   "source": [
    "diz_loss,str_model,NN,validation_loader,_=Run_NN(model=resnet152(False,True),str_model='Resnet152_Norm',num_epochs=100,polar=False,batch_size=20)\n",
    "y_trues,y_preds,conf=get_preds(NN,validation_loader)\n",
    "save_output(diz_loss,str_model,y_trues,y_preds,conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82417200",
   "metadata": {},
   "outputs": [],
   "source": [
    "blurrer =torchvision.transforms.RandomInvert(p=1)\n",
    "\n",
    "\n",
    "with torch.no_grad(): # No need to track the gradients\n",
    "\n",
    "    for image_batch,labels_batch in train_loader:\n",
    "        # Move tensor to the proper device\n",
    "        #polar_transform(image_batch,'linearpolar')\n",
    "        # for label in labels_batch:\n",
    "        #     labels.append(label.numpy())\n",
    "            \n",
    "        for image in image_batch:\n",
    "\n",
    "            plt.imshow(blurrer(image).detach().cpu().numpy()[0],vmin=-7,vmax=7)\n",
    "            print(blurrer(image).detach().cpu().numpy()[0][0,0])\n",
    "            plt.colorbar()\n",
    "            plt.show()\n",
    "\n",
    "            plt.imshow(image.detach().cpu().numpy()[0],vmin=-7,vmax=7)\n",
    "            print(image.detach().cpu().numpy()[0][0,0])\n",
    "            plt.colorbar()\n",
    "            plt.show()\n",
    "            #break\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f48a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "max(diz_loss['val_acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3dffde",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298cdf92",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35033c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "mini=int(round(abs(np.array(images).min()),0))\n",
    "images=images+abs(np.array(images).min())\n",
    "mean=np.array(images).mean()\n",
    "maxi=np.array(images).max()\n",
    "std=np.array(images).std()\n",
    "\n",
    "ratio=255/images.max()\n",
    "images=images*ratio\n",
    "\n",
    "mini=int(mini*ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbcd25f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a82b9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a=np.random.randn(10,10)\n",
    "\n",
    "b=transforms.ToTensor(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc0c089",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b66a46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
