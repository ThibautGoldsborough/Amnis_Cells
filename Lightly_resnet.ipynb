{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72658802",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import required packages\n",
    "import cv2 as cv\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "from Helper_functions import load_dict\n",
    "\n",
    "from AI_functions import resnet18,CellDataset_supervised,data_generator\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e0291f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETS=[\"BoneMarrow_sample1\"]#,\"Retina_1_2\",\"Colon\",\"Choroid\"]\n",
    "DATASET=\"Retina_0_0\"\n",
    "DATASET=\"Retina_1_2\"\n",
    "basepath = r\"C:\\Users\\Thibaut Goldsborough\\Documents\\Seth_BoneMarrow\\Data\\\\\" +DATASET\n",
    "outpath = basepath + \"\\\\Outputs\"\n",
    "image_dim=64 #Dim of the final images\n",
    "nuclear_channel=\"Ch7\"\n",
    "cellmask_channel=\"Ch1_mask\"\n",
    "df=pd.read_csv(outpath+\"\\\\cell_info1.csv\")\n",
    "cell_names=df[\"Cell_ID\"].to_numpy()\n",
    "Prediction_Channels=['Ch07']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5828c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dict=load_dict(outpath,cell_names,image_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b5812a",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dict[6898]['Ch1'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092bef1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(image_dict[6898]['Ch1'],cmap=\"Greys\",vmin=-20,vmax=50)\n",
    "plt.axis('off')\n",
    "plt.savefig(r\"C:\\Users\\Thibaut Goldsborough\\Documents\\Seth_BoneMarrow\\Report\\Icons\\\\\"+\"debris.png\",bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "plt.imshow(image_dict[50199]['Ch1'],cmap=\"Greys\",vmin=-20,vmax=50)\n",
    "plt.axis('off')\n",
    "plt.savefig(r\"C:\\Users\\Thibaut Goldsborough\\Documents\\Seth_BoneMarrow\\Report\\Icons\\\\\"+\"ss.png\",bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "plt.imshow(image_dict[6882]['Ch1'],cmap=\"Greys\",vmin=-20,vmax=50)\n",
    "plt.axis('off')\n",
    "plt.savefig(r\"C:\\Users\\Thibaut Goldsborough\\Documents\\Seth_BoneMarrow\\Report\\Icons\\\\\"+\"dd.png\",bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "plt.imshow(image_dict[939]['Ch1'],cmap=\"Greys\",vmin=-20,vmax=50)\n",
    "plt.axis('off')\n",
    "plt.savefig(r\"C:\\Users\\Thibaut Goldsborough\\Documents\\Seth_BoneMarrow\\Report\\Icons\\\\\"+\"agg.png\",bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "plt.imshow(image_dict[9760]['Ch1'],cmap=\"Greys\",vmin=-20,vmax=50)\n",
    "plt.axis('off')\n",
    "plt.savefig(r\"C:\\Users\\Thibaut Goldsborough\\Documents\\Seth_BoneMarrow\\Report\\Icons\\\\\"+\"photo.png\",bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e9e804",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(image_dict[15000]['Ch1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565a21d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUGS=[]\n",
    "for image in image_dict:\n",
    "    for i in image_dict[image]:\n",
    "      #  print(i)\n",
    "        try: \n",
    "            if i==None:\n",
    "                BUGS.append(image)\n",
    "            \n",
    "        except:\n",
    "            pass\n",
    "\n",
    "print(len(BUGS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67667505",
   "metadata": {},
   "outputs": [],
   "source": [
    "Channels=['Ch1']  #Channel to be fed to the NN\n",
    "images_with_index = []\n",
    "\n",
    "for image_i in image_dict:\n",
    "    #print(image_dict[image_i].keys())\n",
    "    if len(image_dict[image_i].keys())>=len(Channels):\n",
    "        image=cv.merge([image_dict[image_i][i] for i in Channels])\n",
    "        images_with_index.append((int(image_i),image))\n",
    "    else:\n",
    "        print(image_i)\n",
    "    \n",
    "images=np.array([image[1] for image in images_with_index])\n",
    "names=np.array([image[0] for image in images_with_index])\n",
    "assert sum(names!=df['Cell_ID'].to_numpy()) ==0  #Check that the order has been preserved\n",
    "DNA_pos=df[\"DNA_pos\"].to_numpy()\n",
    "Touches_Boundary=df[\"Touches_boundary\"].to_numpy()\n",
    "#labels=df[[\"Intensity_MC_\"+channel for channel in Prediction_Channels]].to_numpy()4\n",
    "labels=df[[\"Scaled_\"+channel for channel in Prediction_Channels]].to_numpy()\n",
    "\n",
    "Thresh=50\n",
    "# plt.hist(df[\"Gradient RMS_M01_Ch01\"],bins=200);\n",
    "# plt.axvline(x=Thresh,color=\"red\")\n",
    "# plt.show()\n",
    "idx_to_keep=np.array(DNA_pos==1,dtype=int)+np.array(Touches_Boundary==0,dtype=int)+np.array(df[\"Gradient RMS_M01_Ch01\"]>Thresh,dtype=int)==3 #keep dnapos, no touch boundarym APC and Other\n",
    "#Filter\n",
    "images=images[idx_to_keep]\n",
    "names=names[idx_to_keep]\n",
    "labels=labels[idx_to_keep]\n",
    "\n",
    "\n",
    "mini=int(round(abs(np.array(images).min()),0))\n",
    "images=images+abs(np.array(images).min())\n",
    "mean=np.array(images).mean()\n",
    "maxi=np.array(images).max()\n",
    "std=np.array(images).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f48493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Scale and normalize the labels for regression\n",
    "# from Helper_functions import log_pol_scale,log_pol\n",
    "# labels=log_pol_scale(labels,slope=1,c=1000)\n",
    "\n",
    "\n",
    "\n",
    "# plt.hist(labels[:,0],bins=100);\n",
    "# plt.show()\n",
    "# #plt.hist(labels[:,1],bins=100);\n",
    "# #plt.show()\n",
    "# #plt.hist(labels[:,2],bins=100);\n",
    "# #print(np.mean(labels,axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81f8917",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(log_pol(labels[:,0],slope=3),bins=100);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3477a318",
   "metadata": {},
   "outputs": [],
   "source": [
    "#del image_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610d6ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#images,labels,names=images[:10000],labels[:10000],names[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59984b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "returned=data_generator(images,labels,names,mini,train_test_split = 0.8,batch_size = 100,sample=False)\n",
    "train,test,batch_size,mean_loader,std_loader=returned\n",
    "[train_data,train_data1,train_labels,train_ID]=train\n",
    "[test_data,test_data1,test_labels,test_ID]=test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d84ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3): \n",
    "    plt.imshow(images[i])\n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97e6fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_mean_std(loader):\n",
    "#         #https://stackoverflow.com/questions/48818619/pytorch-how-do-the-means-and-stds-get-calculated-in-the-transfer-learning-tutor\n",
    "#         mean = 0.\n",
    "#         std = 0.\n",
    "#         for images, _,_ in loader:\n",
    "#             images=images[0]\n",
    "#             batch_samples = images.size(0) # batch size (the last batch can have smaller size!)\n",
    "#             images = images.view(batch_samples, images.size(1), -1)\n",
    "#             mean += images.mean(2).sum(0)\n",
    "#             std += images.std(2).sum(0)\n",
    "\n",
    "#         mean /= len(loader.dataset)\n",
    "#         std /= len(loader.dataset)\n",
    "#         return mean, std\n",
    "\n",
    "\n",
    "\n",
    "# def data_generator(images,labels,names,mini,train_test_split = 0.8,batch_size = 100,sample=False):\n",
    "#     #Split data into training and test, just looking at first images now\n",
    "\n",
    "#     train_data1=images[:int(train_test_split*len(images))]\n",
    "#     test_data1=images[int(train_test_split*len(images)):]\n",
    "\n",
    "#     train_labels=labels[:int(train_test_split*len(images))]\n",
    "#     test_labels=labels[int(train_test_split*len(images)):]\n",
    "\n",
    "#     train_ID=names[:int(train_test_split*len(images))]\n",
    "#     test_ID=names[int(train_test_split*len(images)):]\n",
    "\n",
    "#     #Transform data into tensors, normalize images\n",
    "#     transform_basic = transforms.Compose(\n",
    "#         [transforms.ToPILImage(),\n",
    "#         transforms.ToTensor()\n",
    "#     ])\n",
    "\n",
    "#     train_data_basic = CellDataset_supervised(train_data1,train_labels,train_ID, transform_basic)\n",
    "#     #Create DataLoaders\n",
    "#     train_loader_basic = DataLoader(train_data_basic, batch_size=100, shuffle=True)\n",
    "#     mean_loader,std_loader=get_mean_std(train_loader_basic)\n",
    "\n",
    "\n",
    "#     #Transform data into tensors, normalize images\n",
    "#     transform_train = transforms.Compose(\n",
    "#         [transforms.ToPILImage(),transforms.RandomHorizontalFlip(p=0.5),\n",
    "#         transforms.RandomVerticalFlip(p=0.5),\n",
    "#       #  transforms.RandomRotation(degrees=180,fill=mini),\n",
    "#         transforms.ToTensor(),\n",
    "#     # transforms.Lambda(lambda x: x.repeat(3, 1, 1)),\n",
    "#     transforms.Normalize(mean=[mean_loader], std=[std_loader])  # for grayscale images\n",
    "\n",
    "#     ])\n",
    "\n",
    "#     #Transform data into tensors, normalize images\n",
    "#     transform_test = transforms.Compose(\n",
    "#         [transforms.ToPILImage(),\n",
    "#         transforms.ToTensor(),\n",
    "#         #transforms.Lambda(lambda x: x.repeat(3, 1, 1)),\n",
    "#     transforms.Normalize(mean=[mean_loader], std=[std_loader])  # for grayscale images\n",
    "#     ])\n",
    "\n",
    "#     train_data = CellDataset_supervised(train_data1,train_labels,train_ID, transform_train)\n",
    "#     test_data = CellDataset_supervised(test_data1,test_labels,test_ID, transform_test)\n",
    "\n",
    "\n",
    "\n",
    "#     return [train_data,train_data1,train_labels,train_ID],[test_data,test_data1,test_labels,test_ID],batch_size,mean_loader,std_loader\n",
    "\n",
    "# #Barlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a985a2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose(\n",
    "    [transforms.ToPILImage(),transforms.RandomHorizontalFlip(p=0.5),transforms.RandomVerticalFlip(p=0.5),transforms.RandomRotation(degrees=18,fill=mini),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean_loader,std_loader)  \n",
    "])\n",
    "\n",
    "transform_validation = transforms.Compose(\n",
    "    [transforms.ToPILImage(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean_loader,std_loader)  \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c0e56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform_train  = transforms.Compose([\n",
    "#         transforms.ToPILImage(),\n",
    "#         transforms.RandomRotation(degrees=180,fill=mini),\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Normalize(mean=[mean_loader], std=[std_loader]),  # for grayscale images\n",
    "#         transforms.RandomHorizontalFlip(p=0.5),\n",
    "#         transforms.RandomVerticalFlip(p=0.5),\n",
    "#         transforms.Lambda(lambda x: x if 0.5>np.random.rand() else transforms.functional.invert(x)-1),\n",
    "#         transforms.Lambda(lambda x: x *(1+0.2*np.random.randn()) ),\n",
    "#     # transforms.Lambda(lambda x: x + (0.1**0.5)*torch.randn(64, 64) )\n",
    "#         ]) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd32135",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4b7e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist(train_labels,bins=100);\n",
    "# x=np.linspace(np.min(train_labels),np.max(train_labels),100)\n",
    "# plt.plot(x,sampler_f(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b8a27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = CellDataset_supervised(train_data1,train_labels,train_ID, transform_train)\n",
    "test_data = CellDataset_supervised(test_data1,test_labels, test_ID,transform_validation)\n",
    "\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "print(np.max(train_labels))\n",
    "def sampler_f(x):\n",
    "    return ((x)+abs(np.min(train_labels)))**3+1\n",
    "weights=sampler_f(train_labels)\n",
    "#weights/=np.sum(weights)\n",
    "torch.tensor(weights)\n",
    "sampler = WeightedRandomSampler(weights.T[0], len(weights))\n",
    "\n",
    "\n",
    "#Create DataLoaders\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size,shuffle=False,sampler=sampler)\n",
    "validation_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e7e986",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(train_loader,outpath+\"\\\\\"+DATASET+\"_train.pth\")\n",
    "#torch.save(validation_loader,outpath+\"\\\\\"+DATASET+\"_validation.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7d5945",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(next(iter(train_loader))[0][0][0],vmin=-10,vmax=10)\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "plt.imshow(next(iter(train_loader))[0][0][0],vmin=-10,vmax=10)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90c0631",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(NN, device, dataloader, loss_fn, optimizer,noise_factor=0):\n",
    "    NN.train()\n",
    "    train_loss = []\n",
    "    # Iterate the dataloader (we do not need the label values, this is unsupervised learning)\n",
    "    for image_batch,labels_batch,_ in dataloader: # with \"_\" we just ignore the labels (the second element of the dataloader tuple)\n",
    "        image_noisy = image_batch\n",
    "        image_batch = image_noisy.to(device)\n",
    "        labels_batch=labels_batch.to(device)\n",
    "        output = NN(image_batch)\n",
    "        # Evaluate loss\n",
    "        loss = loss_fn(labels_batch,output)\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss.append(loss.detach().cpu().numpy())\n",
    "    return np.mean(train_loss)\n",
    "\n",
    "### validationing function\n",
    "def validation_epoch(NN, device, dataloader, loss_fn):\n",
    "    NN.eval()\n",
    "    val_loss=[]\n",
    "    with torch.no_grad(): # No need to track the gradients\n",
    "        for image_batch,labels_batch,ID_batch in dataloader:\n",
    "            # Move tensor to the proper device\n",
    "            image_batch = image_batch.to(device)\n",
    "            labels_batch = labels_batch.to(device)\n",
    "            output = NN(image_batch)\n",
    "            loss = loss_fn(labels_batch,output,)\n",
    "            val_loss.append(loss.detach().cpu().numpy())\n",
    "\n",
    "    return np.mean(val_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2200bfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df100=pd.DataFrame()\n",
    "df100['Cell_ID']=np.zeros((len(test_data1)))\n",
    "for channel in Prediction_Channels:\n",
    "    df100[\"AVG_Intensity_MC_\"+channel]=np.zeros((len(test_data1)))\n",
    "\n",
    "def predict(df100):\n",
    "    predictions=np.zeros((len(test_data1),num_classes))\n",
    "    names=np.zeros((len(test_data1)))\n",
    "    labels=np.zeros((len(test_data1),num_classes))\n",
    "    i=0\n",
    "    ConvNet_simple.eval()\n",
    "    with torch.no_grad():\n",
    "        for X, Y,ID in validation_loader:\n",
    "            X = X.to(device)\n",
    "            latents=ConvNet_simple(X).detach().cpu().numpy()\n",
    "            labels[i:i+len(latents)]=Y\n",
    "            names[i:i+len(latents)]=ID\n",
    "            predictions[i:i+len(latents)]=latents\n",
    "            i+=len(latents)\n",
    "\n",
    "    df100['Cell_ID']+=names\n",
    "    for i,channel in enumerate(Prediction_Channels):\n",
    "        df100[\"AVG_Intensity_MC_\"+channel]+=predictions[:,i]\n",
    "\n",
    "    return df100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408100f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes=len(Prediction_Channels)\n",
    "\n",
    "\n",
    "Train_net=torch.load(r'C:\\Users\\Thibaut Goldsborough\\Documents\\Seth_BoneMarrow\\Results\\\\'+\"model300loss_tensor(93.1900).pth\")\n",
    "class net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(net, self).__init__()\n",
    "        self.fc = nn.Linear(2048, 512)\n",
    "        self.fc1 = nn.Linear(512, 512)\n",
    "        self.fc2 = nn.Linear(512, 512)\n",
    "        self.fcf = nn.Linear(512, num_classes)\n",
    "    def forward(self, x):\n",
    "        e = self.fc(x)\n",
    "        e1=self.fc1(e)\n",
    "        e2=self.fc2(e1)\n",
    "        out=self.fcf(e2)\n",
    "        return out\n",
    "\n",
    "Train_net=torch.load(r'C:\\Users\\Thibaut Goldsborough\\Documents\\Seth_BoneMarrow\\Results\\\\'+\"model300loss_tensor(93.1900).pth\")\n",
    "\n",
    "net_add=net()\n",
    "Trained_model = nn.Sequential(Train_net, net_add)  \n",
    "# for name, param in Trained_model.named_parameters():\n",
    "#     if param.requires_grad:print(name)\n",
    "\n",
    "for para in Trained_model[0].parameters():\n",
    "    para.requires_grad = False\n",
    "\n",
    "#Trained_model[1].fc.weight.requires_grad=True\n",
    "\n",
    "#for name, param in Trained_model.named_parameters():\n",
    " #   if param.requires_grad:print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2a79d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.onnx.export(Train_net,X[0][:,None],'rnn.onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa3eb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs=20\n",
    "lr=1e-3\n",
    "num_classes=len(Prediction_Channels)\n",
    "\n",
    "ConvNet_simple=resnet18(channel_num=len(Channels),num_classes=num_classes)\n",
    "#ConvNet_simple=Trained_model\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "ConvNet_simple.to(device)\n",
    "loss_dict={\"L1\":nn.L1Loss(),\"MSE\":nn.MSELoss()}\n",
    "\n",
    "loss_fn = loss_dict[\"L1\"]\n",
    "\n",
    "#optimizer = optim.Adam(ConvNet_simple.parameters(), lr = lr) \n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, ConvNet_simple.parameters()),lr=lr)\n",
    "\n",
    "diz_loss = {'train_loss':[],'val_loss':[]}\n",
    "\n",
    "counter=0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    train_loss = train_epoch(ConvNet_simple,device,train_loader,loss_fn,optimizer)\n",
    "    val_loss = validation_epoch(ConvNet_simple,device,validation_loader,loss_fn)\n",
    "\n",
    "    if epoch>3:\n",
    "        df100=predict(df100)\n",
    "        counter+=1\n",
    "\n",
    "    \n",
    "    print('\\n EPOCH',epoch+1,' \\t train loss',train_loss,' \\t val loss',val_loss)\n",
    "    diz_loss['train_loss'].append(train_loss)\n",
    "    diz_loss['val_loss'].append(val_loss)\n",
    "\n",
    "\n",
    "#_ = loss_over_epochs(diz_loss,num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54db4822",
   "metadata": {},
   "outputs": [],
   "source": [
    "df100=df100/counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0973ddbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(next(iter(train_loader))[0][3][0])\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e946c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df50=pd.DataFrame(diz_loss)\n",
    "df50.to_csv(basepath+\"/Results/Resnet\"+str(diz_loss['train_loss'][-1])+\".csv\", index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86fa8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predictions=np.zeros((len(test_data1),num_classes))\n",
    "names=np.zeros((len(test_data1)))\n",
    "labels=np.zeros((len(test_data1),num_classes))\n",
    "i=0\n",
    "ConvNet_simple.eval()\n",
    "with torch.no_grad():\n",
    "    for X, Y,ID in validation_loader:\n",
    "        X = X.to(device)\n",
    "        latents=ConvNet_simple(X).detach().cpu().numpy()\n",
    "        labels[i:i+len(latents)]=Y\n",
    "        names[i:i+len(latents)]=ID\n",
    "        predictions[i:i+len(latents)]=latents\n",
    "        i+=len(latents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb4955d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Cell_ID\"]=pd.to_numeric(df['Cell_ID'])\n",
    "df10=pd.DataFrame()\n",
    "df10['Cell_ID']=names\n",
    "\n",
    "for i,channel in enumerate(Prediction_Channels):\n",
    "    df10[\"PRED_Intensity_MC_\"+channel]=predictions[:,i]\n",
    "    df10[\"SCALED_Intensity_MC_\"+channel]=labels[:,i]\n",
    "\n",
    "\n",
    "df20=pd.merge(df10,df,on=\"Cell_ID\",how='inner')\n",
    "#df10.to_csv(basepath+\"/Results/Resnet\"+str(diz_loss['train_loss'][-1])+\".csv\", index = False, header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d4163b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scatter(df,channel):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(111)\n",
    "    plt.xlim(np.min(df[\"SCALED_Intensity_MC_\"+channel]),np.max(df[\"SCALED_Intensity_MC_\"+channel]))\n",
    "    plt.ylim(np.min(df[\"SCALED_Intensity_MC_\"+channel]),np.max(df[\"SCALED_Intensity_MC_\"+channel]))\n",
    "    xy=np.linspace(-10,10,5000)\n",
    "    ax.set_aspect('equal', adjustable='box')\n",
    "    x=df[\"SCALED_Intensity_MC_\"+channel]\n",
    "    y=df[\"PRED_Intensity_MC_\"+channel]\n",
    "    plt.scatter(x,y,s=1,alpha=0.5)\n",
    "    print(np.corrcoef(x,y)[0,1])\n",
    "\n",
    "    plt.scatter(xy,xy,s=0.5,alpha=0.2,color=\"red\")\n",
    "    plt.xlabel(\"Actual fluorescence\")\n",
    "    plt.ylabel(\"Predicted fluorescence\")\n",
    "    plt.title(channel)\n",
    "    plt.show()\n",
    "\n",
    "    # fig = plt.figure(figsize=(10,10))\n",
    "    # ax = fig.add_subplot(111)\n",
    "    # plt.xlim(np.min(df[\"SCALED_Intensity_MC_\"+channel]),np.max(df[\"SCALED_Intensity_MC_\"+channel]))\n",
    "    # plt.ylim(np.min(df[\"SCALED_Intensity_MC_\"+channel]),np.max(df[\"SCALED_Intensity_MC_\"+channel]))\n",
    "    # xy=np.linspace(-10,10,5000)\n",
    "    # ax.set_aspect('equal', adjustable='box')\n",
    "    # x=df[\"SCALED_Intensity_MC_\"+channel]\n",
    "    # y=df[\"AVG_Intensity_MC_\"+channel]\n",
    "    # plt.scatter(x,y,s=1,alpha=0.5)\n",
    "    # print(np.corrcoef(x,y)[0,1])\n",
    "\n",
    "    # plt.scatter(xy,xy,s=0.5,alpha=0.2,color=\"red\")\n",
    "    # plt.xlabel(\"Actual fluorescence\")\n",
    "    # plt.ylabel(\"Predicted fluorescence\")\n",
    "    # plt.title(channel)\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc0ff19",
   "metadata": {},
   "outputs": [],
   "source": [
    "df20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d90c12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scatter(df10,'Ch07')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5ae097",
   "metadata": {},
   "outputs": [],
   "source": [
    "df20=pd.merge(df10,df,on=\"Cell_ID\",how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e830d6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f26c4c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "96e92283253362575e1b2577a58171a1def071c2d4840c376515c402fb1735d8"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('deep_learning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
