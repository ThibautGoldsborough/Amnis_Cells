{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72658802",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import required packages\n",
    "import cv2 as cv\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "from Helper_functions import load_dict\n",
    "from AI_functions import resnet18,CellDataset_supervised,data_generator\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1543d3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Insert filepath for local files  FOR THIBAUT\n",
    "basepath = r\"C:\\Users\\Thibaut Goldsborough\\Documents\\Seth_BoneMarrow\\Data\\BoneMarrow_sample1\"\n",
    "readpath = basepath + \"\\\\Raw_Images\"\n",
    "outpath = basepath + \"\\\\Outputs\"\n",
    "file_prefix=\"\\\\sample1_\"\n",
    "maskpath=basepath+\"\\\\ExportedMasks\"\n",
    "image_dim=64 #Dim of the final images\n",
    "nuclear_channel=\"Ch7\"\n",
    "cellmask_channel=\"Ch1_mask\"\n",
    "df=pd.read_csv(outpath+\"\\\\cell_info.csv\")\n",
    "cell_names=df[\"Cell_ID\"].to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5828c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dict=load_dict(outpath,cell_names,image_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67667505",
   "metadata": {},
   "outputs": [],
   "source": [
    "Channels=['Ch1']  #Channel to be fed to the NN\n",
    "images_with_index = []\n",
    "for image_i in image_dict:\n",
    "    image=cv.merge([image_dict[image_i][i] for i in Channels])\n",
    "    images_with_index.append((int(image_i),image))\n",
    "    \n",
    "images=np.array([image[1] for image in images_with_index])\n",
    "names=np.array([image[0] for image in images_with_index])\n",
    "assert sum(names!=df['Cell_ID'].to_numpy()) ==0  #Check that the order has been preserved\n",
    "DNA_pos=df[\"DNA_pos\"].to_numpy()\n",
    "Touches_Boundary=df[\"Touches_boundary\"].to_numpy()\n",
    "\n",
    "labels=df[[\"Intensity_MC_Ch02\",\t\"Intensity_MC_Ch03\"\t,\"Intensity_MC_Ch11\"]].to_numpy()\n",
    "\n",
    "Thresh=50\n",
    "# plt.hist(df[\"Gradient RMS_M01_Ch01\"],bins=200);\n",
    "# plt.axvline(x=Thresh,color=\"red\")\n",
    "# plt.show()\n",
    "idx_to_keep=np.array(DNA_pos==1,dtype=int)+np.array(Touches_Boundary==0,dtype=int)+np.array(df[\"Gradient RMS_M01_Ch01\"]>Thresh,dtype=int)==3 #keep dnapos, no touch boundarym APC and Other\n",
    "#Filter\n",
    "images=images[idx_to_keep]\n",
    "names=names[idx_to_keep]\n",
    "labels=labels[idx_to_keep]\n",
    "\n",
    "\n",
    "mini=int(round(abs(np.array(images).min()),0))\n",
    "images=images+abs(np.array(images).min())\n",
    "mean=np.array(images).mean()\n",
    "maxi=np.array(images).max()\n",
    "std=np.array(images).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f48493",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scale and normalize the labels for regression\n",
    "from Helper_functions import log_pol_scale\n",
    "labels=log_pol_scale(labels)\n",
    "\n",
    "# plt.hist(labels[:,0],bins=100);\n",
    "# plt.show()\n",
    "# plt.hist(labels[:,1],bins=100);\n",
    "# plt.show()\n",
    "# plt.hist(labels[:,2],bins=100);\n",
    "# print(np.mean(labels,axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3477a318",
   "metadata": {},
   "outputs": [],
   "source": [
    "del image_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610d6ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "images,labels,names=images[:1000],labels[:1000],names[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59984b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "returned=data_generator(images,labels,names,mini,train_test_split = 0.8,batch_size = 100,sample=False)\n",
    "train,test,batch_size,mean_loader,std_loader=returned\n",
    "[train_data,train_data1,train_labels,train_ID]=train\n",
    "[test_data,test_data1,test_labels,test_ID]=test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a985a2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose(\n",
    "    [transforms.ToPILImage(),transforms.RandomHorizontalFlip(p=0.5),transforms.RandomVerticalFlip(p=0.5),transforms.RandomRotation(degrees=180,fill=mini),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean_loader,std_loader)  \n",
    "])\n",
    "\n",
    "transform_validation = transforms.Compose(\n",
    "    [transforms.ToPILImage(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean_loader,std_loader)  \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88374da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CellDataset_supervised():\n",
    "    def __init__(self, images,labels,ID, transforms=None):\n",
    "        self.X = images\n",
    "        self.Y=  labels\n",
    "        self.Z= ID\n",
    "        self.transforms = transforms\n",
    "         \n",
    "    def __len__(self):\n",
    "        return (len(self.X))\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        data = self.X[i]\n",
    "        label=self.Y[i]\n",
    "        ID=self.Z[i]\n",
    "        data = np.asarray(data).astype(np.float32)\n",
    "        label = np.asarray(label).astype(np.float32)\n",
    "        \n",
    "        if self.transforms:\n",
    "            data = self.transforms(data)\n",
    "        \n",
    "        return data,label,ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b8a27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = CellDataset_supervised(train_data1,train_labels,train_ID, transform_train)\n",
    "test_data = CellDataset_supervised(test_data1,test_labels, test_ID,transform_validation)\n",
    "\n",
    "\n",
    "#Create DataLoaders\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "validation_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7d5945",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90c0631",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(NN, device, dataloader, loss_fn, optimizer,noise_factor=0):\n",
    "    # Set train mode for both the encoder and the decoder\n",
    "    NN.train()\n",
    "    train_loss = []\n",
    "    total=0\n",
    "    # Iterate the dataloader (we do not need the label values, this is unsupervised learning)\n",
    "    for image_batch,labels_batch,_ in dataloader: # with \"_\" we just ignore the labels (the second element of the dataloader tuple)\n",
    "        image_noisy = image_batch\n",
    "        image_batch = image_noisy.to(device)\n",
    "        labels_batch=labels_batch.to(device)\n",
    "        output = NN(image_batch)\n",
    "        # Evaluate loss\n",
    "        loss = loss_fn(output,labels_batch)\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss.append(loss.detach().cpu().numpy())\n",
    "        total+=labels_batch.size(0)\n",
    "        output=output.detach().cpu().numpy()\n",
    "        labels=labels_batch.detach().cpu().numpy()\n",
    "    return np.mean(train_loss)\n",
    "\n",
    "### validationing function\n",
    "def validation_epoch(NN, device, dataloader, loss_fn):\n",
    "    NN.eval()\n",
    "    total=0\n",
    "    val_loss=[]\n",
    "    with torch.no_grad(): # No need to track the gradients\n",
    "        for image_batch,labels_batch,ID_batch in dataloader:\n",
    "            # Move tensor to the proper device\n",
    "            image_batch = image_batch.to(device)\n",
    "            labels_batch = labels_batch.to(device)\n",
    "            output = NN(image_batch)\n",
    "            loss = loss_fn(output,labels_batch)\n",
    "            total+=labels_batch.size(0)\n",
    "            val_loss.append(loss.detach().cpu().numpy())\n",
    "\n",
    "    return np.mean(val_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8c82e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa3eb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs=1\n",
    "lr=0.001\n",
    "num_classes=3\n",
    "\n",
    "ConvNet_simple=resnet18(channel_num=len(Channels),num_classes=num_classes)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "ConvNet_simple.to(device)\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.Adam(ConvNet_simple.parameters(), lr = lr) \n",
    "\n",
    "diz_loss = {'train_loss':[],'val_loss':[]}\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    train_loss = train_epoch(ConvNet_simple,device,train_loader,loss_fn,optimizer)\n",
    "    val_loss = validation_epoch(ConvNet_simple,device,validation_loader,loss_fn)\n",
    "    \n",
    "    print('\\n EPOCH',epoch+1,' \\t train loss',train_loss,' \\t val loss',val_loss)\n",
    "    diz_loss['train_loss'].append(train_loss)\n",
    "    diz_loss['val_loss'].append(val_loss)\n",
    "\n",
    "\n",
    "#_ = loss_over_epochs(diz_loss,num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86fa8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = CellDataset_supervised(test_data1,test_labels,test_ID)\n",
    "test_loader = DataLoader(test_data, batch_size=124, shuffle=True)#,sampler=sampler)\n",
    "\n",
    "predictions=np.zeros((len(test_data1),num_classes))\n",
    "names=np.zeros((len(test_data1)))\n",
    "labels=np.zeros((len(test_data1),num_classes))\n",
    "i=0\n",
    "ConvNet_simple.eval()\n",
    "with torch.no_grad():\n",
    "    for X, Y,ID in test_loader:\n",
    "        X = X.to(device)\n",
    "        latents=ConvNet_simple(X[:,None]).detach().cpu().numpy()\n",
    "        labels[i:i+len(latents)]=Y\n",
    "        names[i:i+len(latents)]=ID\n",
    "        predictions[i:i+len(latents)]=latents\n",
    "        i+=len(latents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb4955d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Cell_ID\"]=pd.to_numeric(df['Cell_ID'])\n",
    "df10=pd.DataFrame()\n",
    "df10['Cell_ID']=names\n",
    "df10[\"PRED_Intensity_MC_Ch02\"]=predictions[:,0]\n",
    "df10[\"PRED_Intensity_MC_Ch03\"]=predictions[:,1]\n",
    "df10[\"PRED_Intensity_MC_Ch11\"]=predictions[:,2]\n",
    "df10[\"SCALED_Intensity_MC_Ch02\"]=labels[:,0]\n",
    "df10[\"SCALED_Intensity_MC_Ch03\"]=labels[:,1]\n",
    "df10[\"SCALED_Intensity_MC_Ch11\"]=labels[:,2]\n",
    "df10.to_csv(basepath+\"/Results/Resnet\"+str(diz_loss['train_loss'][-1])+\".csv\", index = False, header=True)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "96e92283253362575e1b2577a58171a1def071c2d4840c376515c402fb1735d8"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('deep_learning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
