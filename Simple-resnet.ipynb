{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6074eb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import required packages\n",
    "import cv2 as cv\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader,random_split\n",
    "from torch import nn\n",
    "import torch.nn.functional as Fpython\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4256fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Insert filepath for local files  FOR THIBAUT\n",
    "basepath = r\"C:\\Users\\Thibaut Goldsborough\\Documents\\Seth_BoneMarrow\\Data\\BoneMarrow_smallerfile2\"\n",
    "readpath = basepath + \"\\\\Raw_Images\"\n",
    "outpath = basepath + \"\\\\Outputs\"\n",
    "file_prefix=\"\\\\smaller_file2_\"\n",
    "\n",
    "num_images=10000\n",
    "\n",
    "image_dim=64 #Dim of the final images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414d0a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Insert filepath for local files  FOR THIBAUT\n",
    "basepath = r\"C:\\Users\\Thibaut Goldsborough\\Documents\\Seth_BoneMarrow\\Data\\BoneMarrow_sample1\"\n",
    "readpath = basepath + \"\\\\Raw_Images\"\n",
    "outpath = basepath + \"\\\\Outputs\"\n",
    "file_prefix=\"\\\\sample1_\"\n",
    "maskpath=basepath+\"\\\\ExportedMasks\"\n",
    "\n",
    "\n",
    "\n",
    "image_dim=64 #Dim of the final images\n",
    "\n",
    "nuclear_channel=\"Ch7\"\n",
    "cellmask_channel=\"Ch1_mask\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2324aa3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(outpath+\"\\\\cell_info.csv\")\n",
    "\n",
    "cell_names=df[\"Cell_ID\"].to_numpy()\n",
    "\n",
    "#if sum(df[\"Cell_ID\"].to_numpy()!=cell_names)!=0:\n",
    " #   print(\"Error, dataframe cell ID do not match with entries saved during image processing step\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e258b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dict={}\n",
    "\n",
    "for cell_name in cell_names:\n",
    "    image_dict[cell_name]={}\n",
    "\n",
    "\n",
    "#Find Channels\n",
    "names=[]\n",
    "for entry in os.listdir(outpath): #Read all files\n",
    "    if os.path.isfile(os.path.join(outpath, entry)):\n",
    "        if entry!='image_ID.npy':\n",
    "            names.append(entry)\n",
    "\n",
    "\n",
    "channels=[name[:-4] for name in names if name[-4:]=='.npy']\n",
    "\n",
    "print(\"Channels found:\",channels)\n",
    "\n",
    "data_dict={}\n",
    "for channel in channels:\n",
    "    data_dict[channel]=np.load(outpath+\"\\\\\"+channel+'.npy')\n",
    "\n",
    "#Break up array\n",
    "\n",
    "for channel in data_dict:\n",
    "    dims=data_dict[channel].shape\n",
    "    n=dims[0]//image_dim\n",
    "    l=dims[1]//image_dim\n",
    "    index=0\n",
    "    for i in range(n):\n",
    "        for j in range(l):\n",
    "            img=data_dict[channel][i*image_dim:i*image_dim+image_dim,j*image_dim:j*image_dim+image_dim]\n",
    "            image_dict[cell_names[index]][channel]=img\n",
    "            index+=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099d45db",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cell_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5715850",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_onehot(my_list):\n",
    "    return_list=[]\n",
    "    for i,elem in enumerate(my_list):\n",
    "        j=np.where(np.unique(labels)==elem)\n",
    "        return_list.append(np.zeros((len(np.unique(my_list)))))\n",
    "        return_list[-1][j]=1\n",
    "    return np.array(return_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47184d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dict[0]['Ch1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac96b805",
   "metadata": {},
   "outputs": [],
   "source": [
    "Channels=['Ch1']  #Channel to be fed to the NN\n",
    "\n",
    "images_with_index = []\n",
    "for image_i in image_dict:\n",
    "    image=cv.merge([image_dict[image_i][i] for i in Channels])\n",
    "    images_with_index.append((int(image_i),image))\n",
    "    \n",
    "images_with_index.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7754a93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(images_with_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff8f6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "images=[image[1] for image in images_with_index]\n",
    "names=[image[0] for image in images_with_index]\n",
    "names_arg=np.argsort([image[0] for image in images_with_index])\n",
    "labels=df['Cell_Type'].to_numpy()\n",
    "labels=to_onehot(labels[names_arg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e7e6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mini=int(round(abs(np.array(images).min()),0))\n",
    "images=images+abs(np.array(images).min())\n",
    "mean=np.array(images).mean()\n",
    "maxi=np.array(images).max()\n",
    "std=np.array(images).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4f3e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split data into training and validation, just looking at first images now\n",
    "\n",
    "validation_split = 0.8\n",
    "test_split = 0.9\n",
    "\n",
    "train_end=int(validation_split*len(images))\n",
    "val_end=int(test_split*len(images))\n",
    "\n",
    "train_data1=images[:train_end]\n",
    "validation_data1=images[train_end:val_end]\n",
    "test_data1=images[val_end:]\n",
    "\n",
    "train_labels=labels[:train_end]\n",
    "validation_labels=labels[train_end:val_end]\n",
    "test_labels=labels[val_end:]\n",
    "\n",
    "train_ID=names[:train_end]\n",
    "validation_ID=names[train_end:val_end]\n",
    "test_ID=names[val_end:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73df5cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform data into tensors, normalize images\n",
    "transform_basic = transforms.Compose(\n",
    "    [transforms.ToPILImage(),\n",
    "     transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# custom dataset\n",
    "class CellDataset():\n",
    "    def __init__(self, images,labels, transforms=None):\n",
    "        self.X = images\n",
    "        self.Y=  labels\n",
    "        self.transforms = transforms\n",
    "         \n",
    "    def __len__(self):\n",
    "        return (len(self.X))\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        data = self.X[i]\n",
    "        label=self.Y[i]\n",
    "        data = np.asarray(data).astype(np.uint8)\n",
    "        \n",
    "        if self.transforms:\n",
    "            data = self.transforms(data)\n",
    "        \n",
    "        return data,label\n",
    "\n",
    "\n",
    "train_data_basic = CellDataset(train_data1,train_labels, transform_basic)\n",
    "#Create DataLoaders\n",
    "train_loader_basic = DataLoader(train_data_basic, batch_size=100, shuffle=False)\n",
    "\n",
    "#data=next(iter(train_loader_basic))[0] Don't delete this is useful\n",
    "\n",
    "\n",
    "def get_mean_std(loader):\n",
    "    #https://stackoverflow.com/questions/48818619/pytorch-how-do-the-means-and-stds-get-calculated-in-the-transfer-learning-tutor\n",
    "    mean = 0.\n",
    "    std = 0.\n",
    "    for images, _ in loader:\n",
    "        batch_samples = images.size(0) # batch size (the last batch can have smaller size!)\n",
    "        images = images.view(batch_samples, images.size(1), -1)\n",
    "        mean += images.mean(2).sum(0)\n",
    "        std += images.std(2).sum(0)\n",
    "\n",
    "    mean /= len(loader.dataset)\n",
    "    std /= len(loader.dataset)\n",
    "    return mean, std\n",
    "\n",
    "\n",
    "mean_loader,std_loader=get_mean_std(train_loader_basic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec262f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Free up Space  #Optional\n",
    "# import gc\n",
    "\n",
    "# del images\n",
    "# del shuffle_list\n",
    "# del data_dict\n",
    "# del images_with_index\n",
    "\n",
    "del train_data_basic\n",
    "del train_loader_basic\n",
    "\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3857d8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def polar_transform(images, transform_type='linearpolar'):\n",
    "    images=np.array(images).reshape((len(images),1,60,60))\n",
    "    \"\"\"\n",
    "    This function takes multiple images, and apply polar coordinate conversion to it.\n",
    "    \"\"\"\n",
    "    \n",
    "    (N, C, H, W) = images.shape\n",
    "\n",
    "    for i in range(images.shape[0]):\n",
    "\n",
    "        img = images[i]  # [C,H,W]\n",
    "        img = np.transpose(img, (1, 2, 0))  # [H,W,C]\n",
    "\n",
    "        if transform_type == 'logpolar':\n",
    "            img = cv.logPolar(img, (H // 2, W // 2), W / math.log(W / 2), cv.WARP_FILL_OUTLIERS).reshape(H, W, C)\n",
    "        elif transform_type == 'linearpolar':\n",
    "            img = cv.linearPolar(img, (H // 2, W // 2), W / 2, cv.WARP_FILL_OUTLIERS).reshape(H, W, C)\n",
    "        img = np.transpose(img, (2, 0, 1))\n",
    "\n",
    "        images[i] = torch.from_numpy(img)\n",
    "\n",
    "    return images.reshape((len(images),60,60))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ebb947",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get information about CPU/GPU\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        device = 'cuda:0'\n",
    "    else:\n",
    "        device = 'cpu'\n",
    "    return device\n",
    "device = get_device()\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3aec27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(image,noise_factor):\n",
    "    return image+ noise_factor * torch.randn(*image.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8365c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Type, Any, Callable, Union, List, Optional\n",
    "from torch import Tensor\n",
    "\n",
    "#from .._internally_replaced_utils import load_state_dict_from_url\n",
    "#from ..utils import _log_api_usage_once\n",
    "\n",
    "\n",
    "\n",
    "def conv3x3(in_planes: int, out_planes: int, stride: int = 1, groups: int = 1, dilation: int = 1) -> nn.Conv2d:\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(\n",
    "        in_planes,\n",
    "        out_planes,\n",
    "        kernel_size=3,\n",
    "        stride=stride,\n",
    "        padding=dilation,\n",
    "        groups=groups,\n",
    "        bias=False,\n",
    "        dilation=dilation,\n",
    "    )\n",
    "\n",
    "\n",
    "def conv1x1(in_planes: int, out_planes: int, stride: int = 1) -> nn.Conv2d:\n",
    "    \"\"\"1x1 convolution\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion: int = 1\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        inplanes: int,\n",
    "        planes: int,\n",
    "        stride: int = 1,\n",
    "        downsample: Optional[nn.Module] = None,\n",
    "        groups: int = 1,\n",
    "        base_width: int = 64,\n",
    "        dilation: int = 1,\n",
    "        norm_layer: Optional[Callable[..., nn.Module]] = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        if groups != 1 or base_width != 64:\n",
    "            raise ValueError(\"BasicBlock only supports groups=1 and base_width=64\")\n",
    "        if dilation > 1:\n",
    "            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
    "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = norm_layer(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = norm_layer(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    # Bottleneck in torchvision places the stride for downsampling at 3x3 convolution(self.conv2)\n",
    "    # while original implementation places the stride at the first 1x1 convolution(self.conv1)\n",
    "    # according to \"Deep residual learning for image recognition\"https://arxiv.org/abs/1512.03385.\n",
    "    # This variant is also known as ResNet V1.5 and improves accuracy according to\n",
    "    # https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch.\n",
    "\n",
    "    expansion: int = 4\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        inplanes: int,\n",
    "        planes: int,\n",
    "        stride: int = 1,\n",
    "        downsample: Optional[nn.Module] = None,\n",
    "        groups: int = 1,\n",
    "        base_width: int = 64,\n",
    "        dilation: int = 1,\n",
    "        norm_layer: Optional[Callable[..., nn.Module]] = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        width = int(planes * (base_width / 64.0)) * groups\n",
    "        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv1 = conv1x1(inplanes, width)\n",
    "        self.bn1 = norm_layer(width)\n",
    "        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n",
    "        self.bn2 = norm_layer(width)\n",
    "        self.conv3 = conv1x1(width, planes * self.expansion)\n",
    "        self.bn3 = norm_layer(planes * self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        block: Type[Union[BasicBlock, Bottleneck]],\n",
    "        layers: List[int],\n",
    "        num_classes: int = 4,\n",
    "        zero_init_residual: bool = False,\n",
    "        groups: int = 1,\n",
    "        width_per_group: int = 64,\n",
    "        replace_stride_with_dilation: Optional[List[bool]] = None,\n",
    "        norm_layer: Optional[Callable[..., nn.Module]] = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        #_log_api_usage_once(self)\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        self._norm_layer = norm_layer\n",
    "\n",
    "        self.inplanes = 64\n",
    "        self.dilation = 1\n",
    "        if replace_stride_with_dilation is None:\n",
    "            # each element in the tuple indicates if we should replace\n",
    "            # the 2x2 stride with a dilated convolution instead\n",
    "            replace_stride_with_dilation = [False, False, False]\n",
    "        if len(replace_stride_with_dilation) != 3:\n",
    "            raise ValueError(\n",
    "                \"replace_stride_with_dilation should be None \"\n",
    "                f\"or a 3-element tuple, got {replace_stride_with_dilation}\"\n",
    "            )\n",
    "        self.groups = groups\n",
    "        self.base_width = width_per_group\n",
    "        self.conv1 = nn.Conv2d(len(Channels), self.inplanes, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = norm_layer(self.inplanes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2, dilate=replace_stride_with_dilation[0])\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2, dilate=replace_stride_with_dilation[1])\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2, dilate=replace_stride_with_dilation[2])\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        # Zero-initialize the last BN in each residual branch,\n",
    "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
    "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
    "        if zero_init_residual:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, Bottleneck):\n",
    "                    nn.init.constant_(m.bn3.weight, 0)  # type: ignore[arg-type]\n",
    "                elif isinstance(m, BasicBlock):\n",
    "                    nn.init.constant_(m.bn2.weight, 0)  # type: ignore[arg-type]\n",
    "\n",
    "    def _make_layer(\n",
    "        self,\n",
    "        block: Type[Union[BasicBlock, Bottleneck]],\n",
    "        planes: int,\n",
    "        blocks: int,\n",
    "        stride: int = 1,\n",
    "        dilate: bool = False,\n",
    "    ) -> nn.Sequential:\n",
    "        norm_layer = self._norm_layer\n",
    "        downsample = None\n",
    "        previous_dilation = self.dilation\n",
    "        if dilate:\n",
    "            self.dilation *= stride\n",
    "            stride = 1\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
    "                norm_layer(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(\n",
    "            block(\n",
    "                self.inplanes, planes, stride, downsample, self.groups, self.base_width, previous_dilation, norm_layer\n",
    "            )\n",
    "        )\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(\n",
    "                block(\n",
    "                    self.inplanes,\n",
    "                    planes,\n",
    "                    groups=self.groups,\n",
    "                    base_width=self.base_width,\n",
    "                    dilation=self.dilation,\n",
    "                    norm_layer=norm_layer,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _forward_impl(self, x: Tensor) -> Tensor:\n",
    "        # See note [TorchScript super()]\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self._forward_impl(x)\n",
    "\n",
    "\n",
    "\n",
    "def _resnet(\n",
    "    arch: str,\n",
    "    block: Type[Union[BasicBlock, Bottleneck]],\n",
    "    layers: List[int],\n",
    "    pretrained: bool,\n",
    "    progress: bool,\n",
    "    **kwargs: Any,\n",
    ") -> ResNet:\n",
    "    model = ResNet(block, layers, **kwargs)\n",
    "    if pretrained:\n",
    "        state_dict = load_state_dict_from_url(model_urls[arch], progress=progress)\n",
    "        model.load_state_dict(state_dict)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e7b5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train and evaluate model\n",
    "### Training function\n",
    "def train_epoch(NN, device, dataloader, loss_fn, optimizer,noise_factor=0):\n",
    "    # Set train mode for both the encoder and the decoder\n",
    "    NN.train()\n",
    "    train_loss = []\n",
    "    total=0\n",
    "    correct=0\n",
    "    # Iterate the dataloader (we do not need the label values, this is unsupervised learning)\n",
    "    for image_batch,labels_batch in dataloader: # with \"_\" we just ignore the labels (the second element of the dataloader tuple)\n",
    "        image_noisy = add_noise(image_batch,noise_factor) \n",
    "        image_batch = image_noisy.to(device)\n",
    "        labels_batch=labels_batch.to(device)\n",
    "        output = NN(image_batch)\n",
    "        # Evaluate loss\n",
    "        loss = loss_fn(output,labels_batch)\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss.append(loss.detach().cpu().numpy())\n",
    "\n",
    "        total+=labels_batch.size(0)\n",
    "\n",
    "        output=output.detach().cpu().numpy()\n",
    "        labels=labels_batch.detach().cpu().numpy()\n",
    "\n",
    "        correct+=np.sum(np.argmax(output,axis=1)==np.argmax(labels,axis=1))\n",
    "\n",
    "\n",
    "    accuracy=100 * correct / total\n",
    "\n",
    "    return np.mean(train_loss),accuracy\n",
    "\n",
    "\n",
    "### validationing function\n",
    "def validation_epoch(NN, device, dataloader, loss_fn):\n",
    "    # Set evaluation mode for encoder and decoder\n",
    "    NN.eval()\n",
    "    total=0\n",
    "    correct=0\n",
    "    val_loss=[]\n",
    "\n",
    "    with torch.no_grad(): # No need to track the gradients\n",
    "\n",
    "        for image_batch,labels_batch in dataloader:\n",
    "            # Move tensor to the proper device\n",
    "            image_batch = image_batch.to(device)\n",
    "            labels_batch = labels_batch.to(device)\n",
    "            output = NN(image_batch)\n",
    "            loss = loss_fn(output,labels_batch)\n",
    "\n",
    "            total+=labels_batch.size(0)\n",
    "\n",
    "            output=output.detach().cpu().numpy()\n",
    "            labels=labels_batch.detach().cpu().numpy()\n",
    "\n",
    "            correct+=np.sum(np.argmax(output,axis=1)==np.argmax(labels,axis=1))\n",
    "\n",
    "            val_loss.append(loss.detach().cpu().numpy())\n",
    "\n",
    "    accuracy=100 * correct / total\n",
    "    return np.mean(val_loss),accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26ba1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot losses at the end of training\n",
    "def loss_over_epochs(diz_loss,num_epochs):\n",
    "    plt.figure(figsize=(10,8))\n",
    "    plt.semilogy(diz_loss['train_loss'], label='Train')\n",
    "    plt.semilogy(diz_loss['val_loss'], label='Valid')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Average Loss')\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    plt.title('Loss over ' + str(num_epochs) + ' epochs')\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(10,8))\n",
    "    plt.plot(diz_loss['train_acc'], label='Train')\n",
    "    plt.plot(diz_loss['val_acc'], label='Valid')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Average Accuracy')\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    plt.title('Loss over ' + str(num_epochs) + ' epochs')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d2fbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py\n",
    "\n",
    "\n",
    "def resnet18(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet:\n",
    "    return _resnet(\"resnet18\", BasicBlock, [2, 2, 2, 2], pretrained, progress, **kwargs)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c9da50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Run_NN(train_data=train_data1,validation_data=validation_data1,model=resnet18(False,True),str_model='resnet18',batch_size=20,num_epochs = 25,polar=False,invert=False,lr=0.001):\n",
    "\n",
    "        #Oversampling\n",
    "    #Transform data into tensors, normalize images\n",
    "    transform_train = transforms.Compose(\n",
    "        [transforms.ToPILImage(),transforms.RandomHorizontalFlip(p=0.5),transforms.RandomVerticalFlip(p=0.5),transforms.RandomRotation(degrees=180,fill=mini),\n",
    "       # transforms.GaussianBlur(kernel_size=(3, 3),sigma=(0.5,0.5)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean_loader,std_loader)  \n",
    "    ])\n",
    "\n",
    "    #Transform data into tensors, normalize images\n",
    "    transform_validation = transforms.Compose(\n",
    "        [transforms.ToPILImage(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean_loader,std_loader)  \n",
    "    ])\n",
    "\n",
    "    from torch.utils.data.sampler import WeightedRandomSampler\n",
    "    counts=np.bincount(np.argmax(train_labels,axis=1))\n",
    "    labels_weights = 1. / counts\n",
    "    weights = labels_weights[np.argmax(train_labels,axis=1)]\n",
    "    sampler = WeightedRandomSampler(weights, len(weights))\n",
    "\n",
    "    if invert:\n",
    "        transform_train = transforms.Compose(\n",
    "        [transforms.ToPILImage(),transforms.RandomHorizontalFlip(p=0.5),transforms.RandomVerticalFlip(p=0.5),transforms.RandomRotation(degrees=180,fill=mini),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean_loader,std_loader),\n",
    "        torchvision.transforms.RandomInvert(p=0.5) \n",
    "        ])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    if polar:\n",
    "        transform_train = transforms.Compose(\n",
    "        [transforms.ToPILImage(),\n",
    "        transforms.GaussianBlur(kernel_size=(3, 3),sigma=(0.5,0.5)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean_loader,std_loader)  \n",
    "    ])\n",
    "\n",
    "        train_data=polar_transform(train_data,'logpolar')\n",
    "        validation_data=polar_transform(validation_data,'logpolar')\n",
    "\n",
    "\n",
    "    train_data = CellDataset(train_data,train_labels, transform_train)\n",
    "    validation_data = CellDataset(validation_data,validation_labels, transform_validation)\n",
    "\n",
    "\n",
    "\n",
    "    #Create DataLoaders\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=False,sampler=sampler)\n",
    "    validation_loader = DataLoader(validation_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    ConvNet_simple=model\n",
    "    ConvNet_simple.to(device)\n",
    "    loss_fn = nn.CrossEntropyLoss()   \n",
    "    optimizer = optim.Adam(ConvNet_simple.parameters(), lr = lr) \n",
    "    \n",
    "    diz_loss = {'train_loss':[],'val_loss':[],'train_acc':[],'val_acc':[]}\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        train_loss,train_acc = train_epoch(ConvNet_simple,device,train_loader,loss_fn,optimizer)\n",
    "        val_loss,val_acc = validation_epoch(ConvNet_simple,device,validation_loader,loss_fn)\n",
    "        \n",
    "        print('\\n EPOCH',epoch+1,' \\t train loss',train_loss,' \\t val loss',val_loss,'\\t train acc',train_acc,'\\t val acc',val_acc)\n",
    "        diz_loss['train_loss'].append(train_loss)\n",
    "        diz_loss['val_loss'].append(val_loss)\n",
    "        diz_loss['train_acc'].append(train_acc)\n",
    "        diz_loss['val_acc'].append(val_acc)\n",
    "\n",
    "    _ = loss_over_epochs(diz_loss,num_epochs)\n",
    "\n",
    "    return diz_loss,str_model,ConvNet_simple,validation_loader,train_loader\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2586c77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preds(NN,validation_loader):\n",
    "    y_trues=[]\n",
    "    y_preds=[]\n",
    "\n",
    "\n",
    "    #validation_loader = DataLoader(validation_data, batch_size=int(len(validation_data)/10), shuffle=False)\n",
    "\n",
    "    with torch.no_grad(): # No need to track the gradients\n",
    "\n",
    "        for image_batch,labels_batch in validation_loader:\n",
    "            # Move tensor to the proper device\n",
    "            image_batch = image_batch.to(device)\n",
    "\n",
    "            labels=labels_batch.to(device).detach().cpu().numpy()\n",
    "            y_true=np.argmax(labels,axis=1)\n",
    "\n",
    "            output=NN(image_batch).detach().cpu().numpy()\n",
    "            y_pred=np.argmax(output,axis=1)\n",
    "\n",
    "            y_trues.extend(list(y_true))\n",
    "            y_preds.extend(list(y_pred))     \n",
    "\n",
    "\n",
    "\n",
    "    conf=confusion_matrix(y_trues, y_preds,labels=np.unique(df['Cell_Type'].to_numpy()))\n",
    "\n",
    "    return y_trues,y_preds,conf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2a775e",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=np.argsort(df['Cell_Type'].unique())\n",
    "df['Cell_Type_str'].unique()[a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8fa824",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53285d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(df['Cell_Type_str'].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fe8714",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "#from plotcm import plot_confusion_matrix\n",
    "#plt.figure(figsize=(5,5),dpi=100)\n",
    "##plot_confusion_matrix(conf, [\"Singlet\",\"Doublet\",\"Debris\"],normalize=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3281a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the model outputs\n",
    "\n",
    "def save_output(diz_loss,str_model,y_trues,y_preds,conf):\n",
    "    name=str_model\n",
    "    mean_acc=round(np.mean(diz_loss['val_acc'][-5:]),2)\n",
    "    new_dir=basepath+\"\\\\Results\\\\\"+name+'_'+str(mean_acc)\n",
    "\n",
    "    dir=new_dir\n",
    "    count=1\n",
    "    while os.path.exists(dir):\n",
    "        dir=new_dir+'('+str(count)+')'\n",
    "        count+=1\n",
    "\n",
    "    new_dir=dir\n",
    "    os.mkdir(new_dir)\n",
    "\n",
    "    destination1=new_dir+'\\\\Loss.csv'\n",
    "    destination2=new_dir+'\\\\Predictions.csv'\n",
    "    destination3=new_dir+'\\\\Confusion_mat.png'\n",
    "\n",
    "    df1 = pd.DataFrame.from_dict(diz_loss)\n",
    "    df1.to_csv (destination1, index = False, header=True)\n",
    "\n",
    "    df1 = pd.DataFrame()\n",
    "    df1['Cell_ID']  = validation_ID\n",
    "    df1['Prediction']  = y_preds\n",
    "    df1['Ground Truth']=y_trues\n",
    "    df1.to_csv (destination2, index = False, header=True)\n",
    "\n",
    "    fig=plt.figure(figsize=(5,5),dpi=150)\n",
    "    a=np.argsort(df['Cell_Type'].unique())\n",
    "    list_str=df['Cell_Type_str'].unique()[a]\n",
    "    plot_confusion_matrix(conf, list_str,normalize=True)\n",
    "    fig.savefig(destination3,bbox_inches='tight', dpi=150)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ba9d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "diz_loss,str_model,NN,validation_loader,train_loader=Run_NN(model=resnet18(),str_model='Resnet18',num_epochs=1000,polar=False,batch_size=20)\n",
    "y_trues,y_preds,conf=get_preds(NN,validation_loader)\n",
    "save_output(diz_loss,str_model,y_trues,y_preds,conf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd1850d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "96e92283253362575e1b2577a58171a1def071c2d4840c376515c402fb1735d8"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('deep_learning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
