{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d7f6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import required packages\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mlsauce as ms\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from time import time\n",
    "from os import chdir\n",
    "from sklearn import metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647c7a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET=\"Retina_0_0\"\n",
    "basepath = r\"C:\\Users\\Thibaut Goldsborough\\Documents\\Seth_BoneMarrow\\Data\\\\\" +DATASET\n",
    "outpath = basepath + \"\\\\Outputs\"\n",
    "original_df=pd.read_csv(outpath+\"\\cell_info1.csv\")  \n",
    "original_df=original_df.rename(columns={\"Unnamed: 0\":\"my_Index\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2d5cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "basepath = r\"C:\\Users\\Thibaut Goldsborough\\Documents\\Seth_BoneMarrow\\Data\\\\\" +DATASET\n",
    "outpath = basepath + \"\\\\CellProfiler_Output\"\n",
    "pre_df=pd.read_csv(outpath+\"\\\\BF_cells_on_grid_pre.csv\") \n",
    "\n",
    "df=pd.read_csv(outpath+\"\\BF_cells_on_grid.csv\")  \n",
    "File_raw=df[\"FileName_Marker_image\"].to_numpy()\n",
    "Image_number=df[\"ImageNumber\"].to_numpy()\n",
    "File=np.array([int(i.split(\"Retina_0_0_Ch1\")[1][:-4]) for i in File_raw])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2175085c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This nightmarish code makes sense of the output from cell profiler. This took more than 24h to figure out. \n",
    "\n",
    "large_array=[]\n",
    "for file in np.unique(File):\n",
    "    if file!=11:  #File 11 has two cells with the same index, we ignore file 11 thus removing 1600 cells.\n",
    "        file_locs=np.where(File==file)\n",
    "        s_df=df.iloc[file_locs[0]]\n",
    "        i_num=int(np.unique(Image_number[file_locs[0]]))\n",
    "        s_pre_df=pre_df[pre_df[\"ImageNumber\"]==i_num]\n",
    "        X=s_pre_df[\"Location_Center_X\"].to_numpy()//64\n",
    "        Y=s_pre_df[\"Location_Center_Y\"].to_numpy()//64\n",
    "        Actual_Index=X*16+Y\n",
    "        assert(len(Actual_Index)==len(np.unique(Actual_Index)))\n",
    "        Actual_Index=Actual_Index.astype(int)\n",
    "        Old_Index=s_pre_df[\"Number_Object_Number\"].to_numpy()\n",
    "        new_df=s_df.iloc[:,8:]\n",
    "        features=new_df.to_numpy()\n",
    "        for n,column in enumerate(new_df):\n",
    "           # print(\"File:\",file,\"Column\",n,\"/\",len(new_df.columns))\n",
    "            a=features[:,n]\n",
    "            Actual_Index=np.sort(Actual_Index)\n",
    "            I=np.arange(len(a))\n",
    "            junk=np.where(np.isnan(features[:,0])==1)[0]\n",
    "            II=[]\n",
    "            for i in I:\n",
    "                count=0\n",
    "                if i not in Actual_Index:\n",
    "                  i=junk[count]\n",
    "                  count+=1\n",
    "                  II.append(np.nan)\n",
    "                else:\n",
    "                  II.append(np.float(a[np.where(Actual_Index==i)[0]]))\n",
    "            II=np.array(II)\n",
    "            II=II.reshape(100,16)\n",
    "            II=II.T.flatten()\n",
    "            new_df[column]=II\n",
    "        latest=new_df.to_numpy()\n",
    "        if len(large_array)==0:\n",
    "            large_array=latest\n",
    "        else:\n",
    "            large_array=np.vstack((large_array,latest))\n",
    "\n",
    "X=large_array\n",
    "Y=original_df[\"Scaled_Ch07\"].to_numpy()\n",
    "Y=np.hstack((Y[:11*1600],Y[12*1600:]))  #Skip values corresponding to 11th file\n",
    "ID=original_df[\"Cell_ID\"].to_numpy()\n",
    "ID=np.hstack((ID[:11*1600],ID[12*1600:]))\n",
    "\n",
    "assert len(Y)==len(X)\n",
    "\n",
    "idx_to_keep=np.sum(np.isnan(X),axis=1)==0\n",
    "X=X[idx_to_keep]\n",
    "Y=Y[idx_to_keep]\n",
    "ID=ID[idx_to_keep]\n",
    "\n",
    "final_df=pd.DataFrame()\n",
    "final_df[\"Scaled_Ch07\"]=Y\n",
    "final_df[\"Cell_ID\"]=ID\n",
    "for n,column in enumerate(new_df):\n",
    "    final_df[column]=X[:,n]\n",
    "\n",
    "#final_df.to_csv(basepath+\"\\\\Outputs\"+\"\\\\Cell_features.csv\",index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e16d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "DATASET=\"Retina_0_0\"\n",
    "basepath = r\"C:\\Users\\Thibaut Goldsborough\\Documents\\Seth_BoneMarrow\\Data\\\\\" +DATASET\n",
    "\n",
    "my_df=pd.read_csv(basepath+\"\\\\Outputs\"+\"\\\\Cell_features.csv\")\n",
    "Y=my_df[\"Scaled_Ch07\"]\n",
    "ID=my_df[\"Cell_ID\"]\n",
    "X=my_df.iloc[:,2:].to_numpy()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "X=scaler.transform(X)\n",
    "\n",
    "\n",
    "# rank=np.linalg.matrix_rank(X)\n",
    "# from sklearn.decomposition import PCA\n",
    "# pca = PCA(n_components=rank)\n",
    "# X=pca.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b1def0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y,test_size=0.2)\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "\n",
    "reg = LinearRegression()\n",
    "reg.fit(X_train, y_train)\n",
    "y_pred=reg.predict(X_test)\n",
    "print(reg.score(X_train, y_train))\n",
    "print(reg.score(X_test, y_test))\n",
    "plt.scatter(y_test,y_pred,s=1)\n",
    "print(spearmanr(y_pred,y_test)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d17ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "names=my_df.columns[2:]\n",
    "indexes=[\"Marker\" not in name for name in names]\n",
    "X=X[:,indexes]\n",
    "names=names[indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac33348",
   "metadata": {},
   "outputs": [],
   "source": [
    "import teller as tr\n",
    "plt.figure(figsize=(10,100))\n",
    "explainer = tr.Explainer(obj=reg)\n",
    "explainer.fit(X_test, y_test[:,None], X_names=names, method=\"avg\")\n",
    "explainer.plot(what=\"average_effects\")\n",
    "plt.savefig(basepath+\"\\\\Report\"+\"\\\\CellProfilerfeatures.png\",bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a823e677",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Random Forest\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "print(np.shape(X))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y,test_size=0.2)\n",
    "\n",
    "\n",
    "reg=RandomForestRegressor(n_estimators=100)\n",
    "reg.fit(X_train, y_train)\n",
    "y_pred=reg.predict(X_test)\n",
    "y_train_pred=reg.predict(X_train)\n",
    "print(spearmanr(y_train_pred,y_train)[0])\n",
    "print(spearmanr(y_pred,y_test)[0])\n",
    "\n",
    "#plt.scatter(y_test,y_pred,s=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ceb565b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSBoost\n",
    "\n",
    "\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_boston, load_diabetes\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from time import time\n",
    "from os import chdir\n",
    "from sklearn import metrics\n",
    "import mlsauce as ms\n",
    "\n",
    "\n",
    "print(X.shape,Y.shape)\n",
    "\n",
    "# split data into training test and test set\n",
    "np.random.seed(15029)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, \n",
    "                                                    test_size=0.2)\n",
    "\n",
    "obj = ms.LSBoostRegressor(tolerance=1e-2, activation=\"relu6\", col_sample=1, row_sample=1,n_estimators=50,n_hidden_features=50,dropout=0.5,reg_lambda=0,backend=\"gpu\",learning_rate=0.5)\n",
    "#obj = ms.RidgeRegressor()\n",
    "print(obj.get_params())\n",
    "start = time()\n",
    "obj.fit(X_train, y_train)\n",
    "\n",
    "#print(np.sqrt(np.mean(np.square(obj.predict(X_test) - y_test))))\n",
    "\n",
    "print(\"test\",spearmanr(obj.predict(X_test),y_test)[0])\n",
    "print(\"train\",spearmanr(y_train,obj.predict(X_train))[0])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('deep_learning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "96e92283253362575e1b2577a58171a1def071c2d4840c376515c402fb1735d8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
